{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "20200925_new",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyeon2776/98/blob/master/20200925_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBpq41JKVj6M",
        "colab_type": "text"
      },
      "source": [
        "# **MULTI_IMAGE_CLASSIFICATION**\n",
        "\n",
        "4가지의 이미지 분류\n",
        "\n",
        "기본적인 방법은 단일 이미지 분류와 같다. 대신, 다중 이미지 분류이기 때문에 카테고리의 변화가 있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlpQzeS_9yzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9dee7d3-22c9-4be7-c611-e6b83827ceb0"
      },
      "source": [
        "!pip uninstall keras\n",
        "!pip install Keras==2.2.4\n",
        "\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Keras-2.4.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/Keras-2.4.3.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.6/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Collecting Keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 9.4MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.4.1)\n",
            "Installing collected packages: keras-applications, Keras\n",
            "Successfully installed Keras-2.2.4 keras-applications-1.0.8\n",
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 92kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7raZpBpWzvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5495dea4-b223-4210-c98c-86295dbb6a84"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esqlk-sO4H93",
        "colab_type": "text"
      },
      "source": [
        "### 폴더내 이미지 개수 확인하기 -validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5QppKAj4L7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fbe4018-4ff7-4077-ddaf-464f7349403e"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/test/1. Cancer\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdQlCbkh4WNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f8a3b70-980f-47e4-f271-1ded9c37233b"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/test/2. Precancer\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbfoUq-I4WKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2473e27e-3de5-4623-a5f3-3ebf304e40fe"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/test/3. Extra\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wW2AHTN4n7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17c2b492-0a76-4854-bf6a-1aefa9bda48f"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/test/4. Normal\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXf8Dpe1KeHg",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 부풀리기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXjtASprMCzx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   rotation_range = 90: \n",
        "지정된 각도 범위내에서 임의로 원본이미지를 회전시킵니다. 단위는 도이며, 정수형입니다. 예를 들어 90이라면 0도에서 90도 사이에 임의의 각도로 회전시킵니다. \n",
        "*   width_shift_range = 0.1: \n",
        "지정된 수평방향 이동 범위내에서 임의로 원본이미지를 이동시킵니다. 수치는 전체 넓이의 비율(실수)로 나타냅니다. 예를 들어 0.1이고 전체 넓이가 100이면, 10픽셀 내외로 좌우 이동시킵니다\n",
        "\n",
        "*   height_shift_range = 0.1: \n",
        "지정된 수직방향 이동 범위내에서 임의로 원본이미지를 이동시킵니다. 수치는 전체 높이의 비율(실수)로 나타냅니다. 예를 들어 0.1이고 전체 높이가 100이면, 10픽셀 내외로 상하 이동시킵니다.\n",
        "*   shear_range = 0.5:\n",
        "밀림 강도 범위내에서 임의로 원본이미지를 변형시킵니다. 수치는 시계반대방향으로 밀림 강도를 라디안으로 나타냅니다. 예를 들어 0.5이라면, 0.5 라이안내외로 시계반대방향으로 변형시킵니다.\n",
        "\n",
        "*   zoom_range = 0.3:\n",
        "지정된 확대/축소 범위내에서 임의로 원본이미지를 확대/축소합니다. “1-수치”부터 “1+수치”사이 범위로 확대/축소를 합니다. 예를 들어 0.3이라면, 0.7배에서 1.3배 크기 변화를 시킵니다.\n",
        "*   horizontal_flip = True:\n",
        "수평방향으로 뒤집기를 합니다.\n",
        "*   vertical_flip = True:\n",
        "수직방향으로 뒤집기를 합니다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC2ngZOO4Q3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "0490a17e-1fbc-4f65-c435-bce2f482cb0d"
      },
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array, load_img\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(3)\n",
        "\n",
        "imageGenerator = ImageDataGenerator(rescale=1./255,\n",
        "                                    rotation_range=20,\n",
        "                                    width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    brightness_range=[.2,.2],\n",
        "                                    horizontal_flip=True)\n",
        "                                    #validation_split=.2\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUXM1JyVFX9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "1685cc0e-dc51-4244-a22d-2e28955afb16"
      },
      "source": [
        "#데이터가 충분하면 나누기\n",
        "\n",
        "'''trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "'''                                             \n",
        "\n",
        "'''validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\\n                                                  target_size=(64,64),\\n                                                  subset='validation')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPGhbtSmDVDT",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Cancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqK1bkW6-FlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dad645c2-646d-4e2b-a618-6626f3adae49"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "#path = 이미지resize할 file directory\n",
        "path1 = \"/content/drive/My Drive/CTRC/test/1. Cancer\"\n",
        "\n",
        "file_list = os.listdir(path1)\n",
        "\n",
        "print (\"file_list: {}\".format(file_list))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_list: ['can_1.jpg', 'can_2.jpg', 'can_3.jpg', 'can_4.jpg', 'can_5.jpg', 'can_6.jpg', 'can_8.jpg', 'can_9.jpg', 'can_10.jpg', 'can_11.jpg', 'can_12.jpg', 'can_13.jpg', 'can_14.jpg', 'can_15.jpg', 'can_15_1.jpg', 'can_16.jpg', 'can_17.jpg', 'can_17_1.jpg', 'can_18.jpg', 'can_19.jpg', 'can_20.jpg', 'can_21.jpg', 'can_22.jpg', 'can_22_1.jpg', 'can_23.jpg', 'can_24.jpg', 'can_25.jpg', 'can_26.jpg', 'can_27.jpg', 'can_28.jpg', 'can_30.jpg', 'can_29.jpg', 'can_31.jpg', 'can_32.jpg', 'can_32_1.jpg', 'can_33.jpg', 'can_34.jpg', 'can_35.jpg', 'can_36.jpg', 'can_37.jpg', 'can_37_1.jpg', 'can_38.jpg', 'can_39.jpg', 'can_40.jpg', 'can_40_1.jpg', 'can_41.jpg', 'can_42.jpg', 'can_43.jpg', 'can_44.jpg', 'can_51.jpg', 'can_52.jpg', 'can_53.jpg', 'can_53_1.jpg', 'can_53_2.jpg', 'can_55.jpg', 'can_54.jpg', 'can_57.jpg', 'can_56.jpg', 'can_58.jpg', 'can_59.jpg', 'can_59_1.jpg', 'can_59_2.jpg', 'can_63.jpg', 'can_60.jpg', 'can_60_1.jpg', 'can_62.jpg', 'can_61.jpg', 'can_61_1.jpg', 'can_45.jpg', 'can_46.jpg', 'can_47.jpg', 'can_47_1.jpg', 'can_48.jpg', 'can_49.jpg', 'can_50.jpg', 'can_64.jpg', 'can_65.jpg', 'can_66.jpg', 'can_67.jpg', 'can_67_1.jpg', 'can_67_2.jpg', 'can_68.jpg', 'can_69.jpg', 'can_70.jpg', 'can_71.jpg', 'can_72.jpg', 'can_73.jpg', 'can_74.jpg', 'can_75.jpg', 'can_76.jpg', 'can_77.jpg', 'can_78.jpg', 'can_79.jpg', 'can_80.jpg', 'can_82.jpg', 'can_81.jpg', 'can_83.jpg', 'can_84.jpg', 'can_86.jpg', 'can_85.jpg', 'can_87.jpg', 'can_88.jpg', 'can_89.jpg', 'can_90.jpg', 'can_91.jpg', 'can_92.jpg', 'can_93.jpg', 'can_94.jpg', 'can_95.jpg', 'can_96.jpg', 'can_97.jpg', 'can_97_1.jpg', 'can_98.jpg', 'can_99.jpg', 'can_100.jpg', 'can_101.jpg', 'can_102.jpg', 'can_103.jpg', 'can_104.jpg', 'can_105.jpg', 'can_106.jpg', 'can_107.jpg', 'can_108.jpg', 'can_109.jpg', 'can_110.jpg', 'can_111.jpg', 'can_112.jpg', 'can_113.jpg', 'can_114.jpg', 'can_115.jpg', 'can_116.jpg', 'can_117.jpg', 'can_118.jpg', 'can_120.jpg', 'can_119.jpg', 'can_121.jpg', 'can_122.jpg', 'can_123.jpg', 'can_124.jpg', 'can_125.jpg', 'can_127.jpg', 'can_128.jpg', 'can_129.jpg', 'can_130.jpg', 'can_131.jpg', 'can_132.jpg', 'can_133.jpg', 'can_134.jpg', 'can_135.jpg', 'can_136.jpg', 'can_137.jpg', 'can_138.jpg', 'can_139.jpg', 'can_140.jpg', 'can_141.jpg', 'can_142.jpg', 'can_143.jpg', 'can_144.jpg', 'can_145.jpg', 'can_146.jpg', 'can_147.jpg', 'can_148.jpg', 'can_149.jpg', 'can_150.jpg', 'can_151.jpg', 'can_152.jpg', 'can_153.jpg', 'can_154.jpg', 'can_155.jpg', 'can_156.jpg', 'can_157.jpg', 'can_158.jpg', 'can_159.jpg', 'can_160.jpg', 'can_161.jpg', 'can_162.jpg', 'can_163.jpg', 'can_164.jpg', 'can_165.jpg', 'can_166.jpg', 'can_167.jpg', 'can_168.jpg', 'can_169.jpg', 'can_170.jpg', 'can_171.jpg', 'can_172.jpg', 'can_173.jpg', 'can_174.jpg', 'can_175.jpg', 'can_176.jpg', 'can_177.jpg', 'can_178.jpg', 'can_179.jpg', 'can_180.jpg', 'can_181.jpg', 'can_182.jpg', 'can_183.jpg', 'can_184.jpg', 'can_185.jpg', 'can_186.jpg', 'can_187.jpg', 'can_188.jpg', 'can_189.jpg', 'can_190.jpg', 'can_191.jpg', 'can_192.jpg', 'can_193.jpg', 'can_194.jpg', 'can_195.jpg', 'can_196.jpg', 'can_197.jpg', 'can_198.jpg', 'can_199.jpg', 'can_200.jpg', 'can_201.jpg', 'can_202.jpg', 'can_203.jpg', 'can_204.jpg', 'can_205.jpg', 'can_206.jpg', 'can_207.jpg', 'can_208.jpg', 'can_209.jpg', 'can_210.jpg', 'can_211.jpg', 'can_212.jpg', 'can_213.jpg', 'can_214.jpg', 'can_215.jpg', 'can_216.jpg', 'can_217.jpg', 'can_218.jpg', 'can_219.jpg', 'can_220.jpg', 'can_221.jpg', 'can_222.jpg', 'can_223.jpg', 'can_224.jpg', 'can_225.jpg', 'can_226.jpg', 'can_227.jpg', 'can_228.jpg', 'can_229.jpg', 'can_230.jpg', 'can_231.jpg', 'can_232.jpg', 'can_233.jpg', 'can_234.jpg', 'can_235.jpg', 'can_236.jpg', 'can_237.jpg', 'can_238.jpg', 'can_239.jpg', 'can_240.jpg', 'can_241.jpg', 'can_242.jpg', 'can_243.jpg', 'can_244.jpg', 'can_245.jpg', 'can_246.jpg', 'can_247.jpg', 'can_248.jpg', 'can_249.jpg', 'can_250.jpg', 'can_251.jpg', 'can_252.jpg', 'can_253.jpg', 'can_254.jpg', 'can_255.jpg', 'can_256.jpg', 'can_257.jpg', 'can_258.jpg', 'can_259.jpg', 'can_260.jpg', 'can_261.jpg', 'can_262.jpg', 'can_263.jpg', 'can_264.jpg', 'can_265.jpg', 'can_266.jpg', 'can_267.jpg', 'can_269.jpg', 'can_268.jpg', 'can_270.jpg', 'can_271.jpg', 'can_272.jpg', 'can_273.jpg', 'can_274.jpg', 'can_275.jpg', 'can_276.jpg', 'can_277.jpg', 'can_278.jpg', 'can_279.jpg', 'can_280.jpg', 'can_281.jpg', 'can_282.jpg', 'can_283.jpg', 'can_284.jpg', 'can_285.jpg', 'can_286.jpg', 'can_287.jpg', 'can_288.jpg', 'can_289.jpg', 'can_290.jpg', 'can_291.jpg', 'can_292.jpg', 'can_293.jpg', 'can_294.jpg', 'can_295.jpg', 'can_296.jpg', 'can_297.jpg', 'can_298.jpg', 'can_299.jpg', 'can_300.jpg', 'can_301.jpg', 'can_302.jpg', 'can_303.jpg', 'can_304.jpg', 'can_305.jpg', 'can_306.jpg', 'can_308.jpg', 'can_309.jpg', 'can_310.jpg', 'can_311.jpg', 'can_312.jpg', 'can_313.jpg', 'can_314.jpg', 'can_315.jpg', 'can_316.jpg', 'can_317.jpg', 'can_318.jpg', 'can_319.jpg', 'can_320.jpg', 'can_321.jpg', 'can_322.jpg', 'can_323.jpg', 'can_324.jpg', 'can_325.jpg', 'can_327.jpg', 'can_328.jpg', 'can_329.jpg', 'can_330.jpg', 'can_331.jpg', 'can_332.jpg', 'can_333.jpg', 'can_334.jpg', 'can_335.jpg', 'can_336.jpg', 'can_337.jpg', 'can_338.jpg', 'can_339.jpg', 'can_307.jpg', 'can_340.jpg', 'can_341.jpg', 'can_342.jpg', 'can_343.jpg', 'can_344.jpg', 'can_345.jpg', 'can_346.jpg', 'can_347.jpg', 'can_348.jpg', 'can_326.jpg', 'can_349.jpg', 'can_350.jpg', 'can_351.jpg', 'can_352.jpg', 'can_353.jpg', 'can_354.jpg', 'can_7.jpg', 'can_355.jpg', 'can_356.jpg', 'can_358.jpg', 'can_359.jpg', 'can_360.jpg', 'can_361.jpg', 'can_362.jpg', 'can_363.jpg', 'can_364.jpg', 'can_365.jpg', 'can_366.jpg', 'can_367_1.jpg', 'can_367_2.jpg', 'can_368.jpg', 'can_369_1.jpg', 'can_369_2.jpg', 'can_370.jpg', 'can_371.jpg', 'can_372.jpg', 'can_373.jpg', 'can_374.jpg', 'can_375.jpg', 'can_376.jpg', 'can_377.jpg', 'can_378.jpg', 'can_379.jpg', 'can_380_1.jpg', 'can_380_2.jpg', 'can_381.jpg', 'can_382.jpg', 'can_383.jpg', 'can_384.jpg', 'can_385.jpg', 'can_386.jpg', 'can_387.jpg', 'can_389_1.jpg', 'can_389_2.jpg', 'can_390.jpg', 'can_391.jpg', 'can_357.jpg', 'can_388.jpg', 'can_392.jpg', 'can_393.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjt2R1Oo-W5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8c489605-3b18-484e-d0f8-153f8a5c66aa"
      },
      "source": [
        "n=len(file_list)\n",
        "print(\"n=%d\"%(n))\n",
        "print(file_list[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n=412\n",
            "can_1.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djuHu0ik-dXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "0fba54f7-bc03-45c1-8518-df25362ab77b"
      },
      "source": [
        "np.random.seed(5)\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n):\n",
        "    \n",
        "    img = load_img('/content/drive/My Drive/CTRC/test/1. Cancer/{}'.format(file_list[j]))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for batch in imageGenerator.flow(x, \n",
        "                                     batch_size=1,\n",
        "                                     save_to_dir='/content/drive/My Drive/CTRC/train/1. Cancer', \n",
        "                                     save_prefix='{}'.format(file_list[j]),\n",
        "                                     save_format='jpg'):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i > 13: \n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "  for batch in imageGenerator.flow(x, batch_size=1,\n",
        "                              save_to_dir=saveDir,\n",
        "                              save_format='jpg'):\n",
        "        i += 1'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 16647 images belonging to 4 classes.\n",
            "Found 0 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-93ed6c6d712e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                                      \u001b[0msave_to_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/CTRC/train/1. Cancer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                      \u001b[0msave_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                      save_format='jpg'):\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     format=self.save_format)\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mbatch_x_miscs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_misc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         output = (batch_x if batch_x_miscs == []\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2097\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn9l8z9J_PFQ",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Precancer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWk-j42J_MQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "#path = 이미지resize할 file directory\n",
        "path2 = \"/content/drive/My Drive/CTRC/test/2. Precancer\"\n",
        "\n",
        "file_list = os.listdir(path2)\n",
        "\n",
        "print (\"file_list: {}\".format(file_list))\n",
        "\n",
        "n=len(file_list)\n",
        "print(\"n=%d\"%(n))\n",
        "print(file_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELEVzorm_RnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(5)\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train/2. Precancer',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test/2. Precancer',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n):\n",
        "    \n",
        "    img = load_img('/content/drive/My Drive/CTRC/test/2. Precancer/{}'.format(file_list[j]))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for batch in imageGenerator.flow(x, batch_size=1, save_to_dir='/content/drive/My Drive/CTRC/train/2. Precancer',\n",
        "                               save_prefix='{}'.format(file_list[j]), save_format='jpg'):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i >35: \n",
        "\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEdhTRFuDXt6",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM2DjGJPDZke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "#path = 이미지resize할 file directory\n",
        "path3 = \"/content/drive/My Drive/CTRC/test/3. Extra\"\n",
        "\n",
        "file_list = os.listdir(path3)\n",
        "\n",
        "print (\"file_list: {}\".format(file_list))\n",
        "\n",
        "n=len(file_list)\n",
        "print(\"n=%d\"%(n))\n",
        "print(file_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rGQ0gGbDeIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(5)\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train/3. Extra',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test/3. Extra',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n):\n",
        "    \n",
        "    img = load_img('/content/drive/My Drive/CTRC/test/3. Extra/{}'.format(file_list[j]))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for batch in imageGenerator.flow(x, batch_size=1, save_to_dir='/content/drive/My Drive/CTRC/train/3. Extra',\n",
        "                               save_prefix='{}'.format(file_list[j]), save_format='jpg'):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i > 20: \n",
        "\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F347m1zsDpIO",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Es_QupIDoyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "#path = 이미지resize할 file directory\n",
        "path4 = \"/content/drive/My Drive/CTRC/test/4. Normal\"\n",
        "\n",
        "file_list = os.listdir(path4)\n",
        "\n",
        "print (\"file_list: {}\".format(file_list))\n",
        "\n",
        "n=len(file_list)\n",
        "print(\"n=%d\"%(n))\n",
        "print(file_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Y99gTdDvQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(5)\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train/4. Normal',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test/4. Normal',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n):\n",
        "    \n",
        "    img = load_img('/content/drive/My Drive/CTRC/test/4. Normal/{}'.format(file_list[j]))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for batch in imageGenerator.flow(x, batch_size=1, save_to_dir='/content/drive/My Drive/CTRC/train/4. Normal',\n",
        "                               save_prefix='{}'.format(file_list[j]), save_format='jpg'):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i > 5: \n",
        "\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ArNio6RRgTe",
        "colab_type": "text"
      },
      "source": [
        "### 폴더내 이미지 개수 확인하기 -train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XvzMI44RjGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#코드 추가해서 확인하기\n",
        "\n",
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/train/4. Normal\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7mC-hEeTsAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "3bc0cba9-ed2f-4e8e-a4f0-72d888e1a096"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/train/3. Extra\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2879"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W29rppybTr-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "d3657b4c-9b32-4acd-8e2b-7bbf402306af"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/train/2. Precancer\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_eyxexgT4V9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "9eb00a49-c72f-4ea5-a062-d1413e6c43b0"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/train/1. Cancer\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5_HvWmEda9r",
        "colab_type": "text"
      },
      "source": [
        "# 이지우 설계\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpIHpYP_dwBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = [\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"]\n",
        "nb_classes = len(categories)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZdm7Q0ZdZZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "25772587-6a75-4fa5-d34f-2a11b2a98b79"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "caltech_dir = '/content/drive/My Drive/CTRC/train'\n",
        "categories = [\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"]\n",
        "nb_classes = len(categories)\n",
        "\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for idx, cat in enumerate(categories):\n",
        "    \n",
        "    #one-hot 돌리기.\n",
        "    label = [0 for i in range(nb_classes)]\n",
        "    label[idx] = 1\n",
        "\n",
        "    image_dir = caltech_dir + \"/\" + cat\n",
        "    files = glob.glob(image_dir+\"/*.jpg\")\n",
        "    print(cat, \" 파일 길이 : \", len(files))\n",
        "    for i, f in enumerate(files):\n",
        "        img = Image.open(f)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img = img.resize((image_w, image_h))\n",
        "        data = np.asarray(img)\n",
        "\n",
        "        X.append(data)\n",
        "        y.append(label)\n",
        "\n",
        "        if i % 700 == 0:\n",
        "            print(cat, \" : \", f)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "#1 0 0 0 이면 airplanes\n",
        "#0 1 0 0 이면 buddha 이런식\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=321)\n",
        "xy = (X_train, X_test, y_train, y_test)\n",
        "np.save('/content/drive/My Drive/CTRC/train/multi_image_data.npy', xy)\n",
        "\n",
        "print(\"ok\", len(y))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Cancer  파일 길이 :  2471\n",
            "1. Cancer  :  /content/drive/My Drive/CTRC/train/1. Cancer/can_232.jpg_0_9397.jpg\n",
            "1. Cancer  :  /content/drive/My Drive/CTRC/train/1. Cancer/can_349.jpg_0_240.jpg\n",
            "1. Cancer  :  /content/drive/My Drive/CTRC/train/1. Cancer/can_132.jpg_0_9914.jpg\n",
            "1. Cancer  :  /content/drive/My Drive/CTRC/train/1. Cancer/can_17.jpg_0_4612.jpg\n",
            "2. Precancer  파일 길이 :  1797\n",
            "2. Precancer  :  /content/drive/My Drive/CTRC/train/2. Precancer/precan_67.jpg_0_9867.jpg\n",
            "2. Precancer  :  /content/drive/My Drive/CTRC/train/2. Precancer/precan_123.jpg_0_2662.jpg\n",
            "2. Precancer  :  /content/drive/My Drive/CTRC/train/2. Precancer/precan_34.jpg_0_8027.jpg\n",
            "3. Extra  파일 길이 :  2879\n",
            "3. Extra  :  /content/drive/My Drive/CTRC/train/3. Extra/inf_160.jpg_0_2318.jpg\n",
            "3. Extra  :  /content/drive/My Drive/CTRC/train/3. Extra/inf_229.jpg_0_7959.jpg\n",
            "3. Extra  :  /content/drive/My Drive/CTRC/train/3. Extra/inf_107.jpg_0_1857.jpg\n",
            "3. Extra  :  /content/drive/My Drive/CTRC/train/3. Extra/inf_8.jpg_0_4612.jpg\n",
            "3. Extra  :  /content/drive/My Drive/CTRC/train/3. Extra/inf_63.jpg_0_7942.jpg\n",
            "4. Normal  파일 길이 :  6821\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_314_2.jpg_0_1991.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_89_1.jpg_0_2121.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_295.jpg_0_412.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_248.jpg_0_4757.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_268_3.jpg_0_5038.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_232_4.jpg_0_4848.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_429_2.jpg_0_3941.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_214_2.jpg_0_5897.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_396_1.jpg_0_7886.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_180.jpg_0_3230.jpg\n",
            "ok 13968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmPn9lZpdhMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "2f5222fa-3445-4c74-a916-3d65c138ebd5"
      },
      "source": [
        "import os, glob, numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend.tensorflow_backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "\n",
        "X_train, X_test, y_train, y_test = np.load('/content/drive/My Drive/CTRC/train/multi_image_data.npy', \n",
        "                                           allow_pickle=True)\n",
        "\n",
        "\n",
        "#일반화\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_test = X_test.astype(float) / 255\n",
        "\n",
        "print(len(y_train))\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(y_test))\n",
        "print(X_train.shape)\n",
        "print(X_train.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9777\n",
            "9777\n",
            "4191\n",
            "4191\n",
            "(9777, 64, 64, 3)\n",
            "9777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9352Bt6Inuk7",
        "colab_type": "text"
      },
      "source": [
        "## CNN 모델 만들기 (Conv2D 이용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhc9w51LVj7s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "3765c4d6-1a4b-4a4e-a8df-b1d7e246c5ab"
      },
      "source": [
        "#import keras.backend.tensorflow_backend as K\n",
        "import tensorflow.keras.backend as K\n",
        "import os, glob, numpy as np\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (7,7), padding=\"same\", input_shape=(64,64,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "    # 3X3 크기의 컨볼루션 레이어를 32개의 필터수를 처음에 생성\n",
        "    # 활성화 함수 relu\n",
        "    # (64,64,3)의 튜플 값 가진다\n",
        "    # Maxpooling2D를 통해 중요 값만 뽑아 작은 출력값 만든다\n",
        "\n",
        "    \n",
        "model.add(Conv2D(64, (5,5), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#odel.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1,1)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(512, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1,1)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Flatten()) \n",
        "    #CNN에서 컨볼루션 레이어나 맥스풀링을 거치면 주요 특징만 추출되어 학습됨\n",
        "    \n",
        "    #컨볼루션이나 맥스풀링은 주로 2차원을 다루지만\n",
        "    #전결합층에 전달을 하기 위해서는 1차원으로 바꿔야하는데\n",
        "    #이 때 Flatten사용\n",
        "model.add(Dense(256, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.15))\n",
        "\n",
        "    #model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=['accuracy'])\n",
        "model_dir = './model'\n",
        "    \n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "    \n",
        "model_path = model_dir + '/multi_img_classification.model'\n",
        "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATW202c9Q6Ct",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])# 코드로 형식 지정됨\n",
        "```\n",
        "\n",
        "\n",
        "-손실 함수(Loss function)-훈련 하는 동안 모델의 오차를 측정합니다. 모델의 학습이 올바른 방향으로 향하도록 이 함수를 최소화해야 합니다.\n",
        "\n",
        "-옵티마이저(Optimizer)-데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정합니다.\n",
        "\n",
        "-지표(Metrics)-훈련 단계와 테스트 단계를 모니터링하기 위해 사용합니다. 다음 예에서는 올바르게 분류된 이미지의 비율인 정확도를 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LISq6dNrZJs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "model.add(Dropout(0.25))\n",
        "```\n",
        "\n",
        "\n",
        "오버피팅을 피하기 위하여 25% 드롭아웃\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6RTeFURre1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFPqoC_nVj8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67aac133-49da-4b93-be5d-414a3ceca460"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 64, 64, 32)        4736      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 1, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4)                 36        \n",
            "=================================================================\n",
            "Total params: 2,530,676\n",
            "Trainable params: 2,530,676\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_S9grYCWkbB",
        "colab_type": "text"
      },
      "source": [
        "드롭아웃 0.1두번 0.15두번"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74PccX8coNHb",
        "colab_type": "text"
      },
      "source": [
        "## model.fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D-yBz5VKRdO",
        "colab_type": "text"
      },
      "source": [
        "**model.fit_generator**\n",
        "\n",
        "- 첫번째 인자 : 훈련데이터셋을 제공할 제네레이터를 지정합니다.\n",
        "\n",
        "- steps_per_epoch : 한 epoch에 사용한 스텝 수를 지정합니다. \n",
        "\n",
        "- epochs : 전체 훈련 데이터셋에 대해 학습 반복 횟수를 지정합니다. \n",
        "\n",
        "- validation_data : 검증데이터셋을 제공할 제네레이터를 지정합니다. \n",
        "\n",
        "- validation_steps : 한 epoch 종료 시 마다 검증할 때 사용되는 검증 스텝 수를 지정합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZcHM-pPXg5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "da11e1d9-92aa-41f2-ed5c-a1e1093fb462"
      },
      "source": [
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  batch_size=3,\n",
        "                                                   class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1961 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUfA4nyKh51L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18cdb77d-e845-4819-8355-e1b3b466e5f9"
      },
      "source": [
        "\n",
        "history = model.fit(X_train, y_train, \n",
        "                    batch_size=300, epochs=100, \n",
        "                    validation_data=(X_test, y_test))\n",
        "\n",
        "\n",
        "'''#Train\n",
        "history=model.fit_generator(trainGen,\n",
        "                    steps_per_epoch=300,\n",
        "                    epochs=100,\n",
        "                    validation_data=(X_test, y_test) \n",
        "                    validation_steps=60)'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 9777 samples, validate on 4191 samples\n",
            "Epoch 1/100\n",
            "9777/9777 [==============================] - 293s 30ms/step - loss: 1.3345 - acc: 0.4660 - val_loss: 1.2786 - val_acc: 0.4803\n",
            "Epoch 2/100\n",
            "9777/9777 [==============================] - 283s 29ms/step - loss: 1.2522 - acc: 0.4877 - val_loss: 1.2515 - val_acc: 0.4803\n",
            "Epoch 3/100\n",
            "9777/9777 [==============================] - 274s 28ms/step - loss: 1.1980 - acc: 0.4903 - val_loss: 1.1415 - val_acc: 0.4803\n",
            "Epoch 4/100\n",
            "9777/9777 [==============================] - 275s 28ms/step - loss: 1.1386 - acc: 0.5126 - val_loss: 1.0976 - val_acc: 0.5364\n",
            "Epoch 5/100\n",
            "9777/9777 [==============================] - 270s 28ms/step - loss: 1.1086 - acc: 0.5362 - val_loss: 1.0806 - val_acc: 0.5402\n",
            "Epoch 6/100\n",
            "9777/9777 [==============================] - 268s 27ms/step - loss: 1.0958 - acc: 0.5317 - val_loss: 1.0603 - val_acc: 0.4915\n",
            "Epoch 7/100\n",
            "9777/9777 [==============================] - 267s 27ms/step - loss: 1.0728 - acc: 0.5351 - val_loss: 1.0157 - val_acc: 0.5414\n",
            "Epoch 8/100\n",
            "9777/9777 [==============================] - 267s 27ms/step - loss: 1.0441 - acc: 0.5497 - val_loss: 0.9913 - val_acc: 0.5481\n",
            "Epoch 9/100\n",
            "9777/9777 [==============================] - 270s 28ms/step - loss: 1.0334 - acc: 0.5543 - val_loss: 0.9760 - val_acc: 0.5617\n",
            "Epoch 10/100\n",
            "9777/9777 [==============================] - 269s 28ms/step - loss: 1.0196 - acc: 0.5543 - val_loss: 0.9895 - val_acc: 0.5665\n",
            "Epoch 11/100\n",
            "9777/9777 [==============================] - 267s 27ms/step - loss: 0.9999 - acc: 0.5607 - val_loss: 1.0256 - val_acc: 0.5829\n",
            "Epoch 12/100\n",
            "9777/9777 [==============================] - 268s 27ms/step - loss: 0.9673 - acc: 0.5780 - val_loss: 0.9356 - val_acc: 0.5827\n",
            "Epoch 13/100\n",
            "9777/9777 [==============================] - 271s 28ms/step - loss: 0.9369 - acc: 0.5814 - val_loss: 0.9217 - val_acc: 0.5960\n",
            "Epoch 14/100\n",
            "9777/9777 [==============================] - 266s 27ms/step - loss: 0.9152 - acc: 0.5871 - val_loss: 1.0034 - val_acc: 0.5846\n",
            "Epoch 15/100\n",
            "9777/9777 [==============================] - 268s 27ms/step - loss: 0.9467 - acc: 0.5741 - val_loss: 0.9083 - val_acc: 0.5786\n",
            "Epoch 16/100\n",
            "9777/9777 [==============================] - 266s 27ms/step - loss: 0.9079 - acc: 0.5792 - val_loss: 0.9007 - val_acc: 0.6006\n",
            "Epoch 17/100\n",
            "9777/9777 [==============================] - 266s 27ms/step - loss: 0.8830 - acc: 0.6047 - val_loss: 0.8606 - val_acc: 0.6421\n",
            "Epoch 18/100\n",
            "9777/9777 [==============================] - 266s 27ms/step - loss: 0.8669 - acc: 0.6223 - val_loss: 0.8547 - val_acc: 0.6407\n",
            "Epoch 19/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.8751 - acc: 0.6204 - val_loss: 0.8594 - val_acc: 0.6271\n",
            "Epoch 20/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.8208 - acc: 0.6391 - val_loss: 0.9210 - val_acc: 0.6180\n",
            "Epoch 21/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.8179 - acc: 0.6370 - val_loss: 0.8129 - val_acc: 0.6617\n",
            "Epoch 22/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.7893 - acc: 0.6513 - val_loss: 0.7904 - val_acc: 0.6590\n",
            "Epoch 23/100\n",
            "9777/9777 [==============================] - 271s 28ms/step - loss: 0.7567 - acc: 0.6585 - val_loss: 0.8153 - val_acc: 0.6781\n",
            "Epoch 24/100\n",
            "9777/9777 [==============================] - 265s 27ms/step - loss: 0.7736 - acc: 0.6569 - val_loss: 0.8144 - val_acc: 0.6800\n",
            "Epoch 25/100\n",
            "9777/9777 [==============================] - 266s 27ms/step - loss: 0.7478 - acc: 0.6666 - val_loss: 0.8356 - val_acc: 0.6602\n",
            "Epoch 26/100\n",
            "9777/9777 [==============================] - 265s 27ms/step - loss: 0.7322 - acc: 0.6691 - val_loss: 0.8252 - val_acc: 0.6681\n",
            "Epoch 27/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.7056 - acc: 0.6851 - val_loss: 0.7728 - val_acc: 0.6819\n",
            "Epoch 28/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.6996 - acc: 0.6882 - val_loss: 0.7654 - val_acc: 0.6700\n",
            "Epoch 29/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.7698 - acc: 0.6637 - val_loss: 0.9427 - val_acc: 0.6290\n",
            "Epoch 30/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.7547 - acc: 0.6672 - val_loss: 0.7735 - val_acc: 0.6769\n",
            "Epoch 31/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.6764 - acc: 0.7009 - val_loss: 0.7591 - val_acc: 0.6764\n",
            "Epoch 32/100\n",
            "9777/9777 [==============================] - 266s 27ms/step - loss: 0.6463 - acc: 0.7052 - val_loss: 0.7274 - val_acc: 0.6881\n",
            "Epoch 33/100\n",
            "9777/9777 [==============================] - 265s 27ms/step - loss: 0.6167 - acc: 0.7188 - val_loss: 0.7715 - val_acc: 0.6886\n",
            "Epoch 34/100\n",
            "9777/9777 [==============================] - 267s 27ms/step - loss: 0.6503 - acc: 0.7067 - val_loss: 0.7610 - val_acc: 0.7005\n",
            "Epoch 35/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.6119 - acc: 0.7164 - val_loss: 0.7347 - val_acc: 0.6943\n",
            "Epoch 36/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.6589 - acc: 0.7077 - val_loss: 0.7887 - val_acc: 0.6657\n",
            "Epoch 37/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.6474 - acc: 0.7160 - val_loss: 0.8300 - val_acc: 0.7063\n",
            "Epoch 38/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.5673 - acc: 0.7419 - val_loss: 0.7235 - val_acc: 0.6977\n",
            "Epoch 39/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.5431 - acc: 0.7538 - val_loss: 1.0894 - val_acc: 0.6936\n",
            "Epoch 40/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.5429 - acc: 0.7475 - val_loss: 0.7242 - val_acc: 0.7218\n",
            "Epoch 41/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.5157 - acc: 0.7622 - val_loss: 0.7929 - val_acc: 0.7060\n",
            "Epoch 42/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.5547 - acc: 0.7528 - val_loss: 0.9892 - val_acc: 0.7025\n",
            "Epoch 43/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.4937 - acc: 0.7777 - val_loss: 0.8005 - val_acc: 0.7165\n",
            "Epoch 44/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.4734 - acc: 0.7887 - val_loss: 0.8935 - val_acc: 0.7127\n",
            "Epoch 45/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.5046 - acc: 0.7741 - val_loss: 0.8129 - val_acc: 0.6960\n",
            "Epoch 46/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.4964 - acc: 0.7882 - val_loss: 0.7881 - val_acc: 0.7385\n",
            "Epoch 47/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.4514 - acc: 0.8088 - val_loss: 0.7771 - val_acc: 0.7366\n",
            "Epoch 48/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.4347 - acc: 0.8136 - val_loss: 0.9271 - val_acc: 0.6884\n",
            "Epoch 49/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.4192 - acc: 0.8199 - val_loss: 0.7830 - val_acc: 0.7511\n",
            "Epoch 50/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.4076 - acc: 0.8316 - val_loss: 0.8788 - val_acc: 0.7466\n",
            "Epoch 51/100\n",
            "9777/9777 [==============================] - 270s 28ms/step - loss: 0.4163 - acc: 0.8266 - val_loss: 0.8475 - val_acc: 0.7523\n",
            "Epoch 52/100\n",
            "9777/9777 [==============================] - 272s 28ms/step - loss: 0.4313 - acc: 0.8242 - val_loss: 0.8493 - val_acc: 0.6793\n",
            "Epoch 53/100\n",
            "9777/9777 [==============================] - 267s 27ms/step - loss: 0.4610 - acc: 0.8087 - val_loss: 0.8581 - val_acc: 0.7402\n",
            "Epoch 54/100\n",
            "9777/9777 [==============================] - 265s 27ms/step - loss: 0.4291 - acc: 0.8254 - val_loss: 0.8635 - val_acc: 0.7366\n",
            "Epoch 55/100\n",
            "9777/9777 [==============================] - 266s 27ms/step - loss: 0.3660 - acc: 0.8480 - val_loss: 0.8926 - val_acc: 0.7578\n",
            "Epoch 56/100\n",
            "9777/9777 [==============================] - 264s 27ms/step - loss: 0.3685 - acc: 0.8551 - val_loss: 0.9614 - val_acc: 0.7471\n",
            "Epoch 57/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.3199 - acc: 0.8727 - val_loss: 0.9606 - val_acc: 0.7647\n",
            "Epoch 58/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.2842 - acc: 0.8906 - val_loss: 1.0104 - val_acc: 0.7712\n",
            "Epoch 59/100\n",
            "9777/9777 [==============================] - 262s 27ms/step - loss: 0.2850 - acc: 0.8922 - val_loss: 1.0833 - val_acc: 0.7745\n",
            "Epoch 60/100\n",
            "9777/9777 [==============================] - 261s 27ms/step - loss: 0.3024 - acc: 0.8804 - val_loss: 0.8629 - val_acc: 0.7712\n",
            "Epoch 61/100\n",
            "9777/9777 [==============================] - 263s 27ms/step - loss: 0.3075 - acc: 0.8858 - val_loss: 1.0338 - val_acc: 0.7454\n",
            "Epoch 62/100\n",
            "9777/9777 [==============================] - 280s 29ms/step - loss: 0.3083 - acc: 0.8841 - val_loss: 0.9655 - val_acc: 0.7776\n",
            "Epoch 63/100\n",
            "9777/9777 [==============================] - 280s 29ms/step - loss: 0.3296 - acc: 0.8810 - val_loss: 0.9311 - val_acc: 0.7483\n",
            "Epoch 64/100\n",
            "9777/9777 [==============================] - 279s 28ms/step - loss: 0.3434 - acc: 0.8696 - val_loss: 1.0166 - val_acc: 0.7581\n",
            "Epoch 65/100\n",
            "9777/9777 [==============================] - 284s 29ms/step - loss: 0.2837 - acc: 0.8955 - val_loss: 0.8869 - val_acc: 0.7628\n",
            "Epoch 66/100\n",
            "9777/9777 [==============================] - 277s 28ms/step - loss: 0.3763 - acc: 0.8562 - val_loss: 0.8649 - val_acc: 0.7702\n",
            "Epoch 67/100\n",
            "9777/9777 [==============================] - 271s 28ms/step - loss: 0.2354 - acc: 0.9131 - val_loss: 0.9653 - val_acc: 0.8017\n",
            "Epoch 68/100\n",
            "9777/9777 [==============================] - 274s 28ms/step - loss: 0.2122 - acc: 0.9261 - val_loss: 1.2854 - val_acc: 0.7721\n",
            "Epoch 69/100\n",
            "9777/9777 [==============================] - 274s 28ms/step - loss: 0.2972 - acc: 0.8910 - val_loss: 1.0368 - val_acc: 0.7588\n",
            "Epoch 70/100\n",
            "9777/9777 [==============================] - 275s 28ms/step - loss: 0.2416 - acc: 0.9136 - val_loss: 0.9994 - val_acc: 0.7731\n",
            "Epoch 71/100\n",
            "9777/9777 [==============================] - 276s 28ms/step - loss: 0.2853 - acc: 0.8895 - val_loss: 1.1452 - val_acc: 0.7748\n",
            "Epoch 72/100\n",
            "9777/9777 [==============================] - 290s 30ms/step - loss: 0.4532 - acc: 0.8436 - val_loss: 1.1612 - val_acc: 0.6943\n",
            "Epoch 73/100\n",
            "9777/9777 [==============================] - 292s 30ms/step - loss: 0.4609 - acc: 0.8302 - val_loss: 0.9201 - val_acc: 0.7759\n",
            "Epoch 74/100\n",
            "9777/9777 [==============================] - 292s 30ms/step - loss: 0.2667 - acc: 0.9055 - val_loss: 1.1728 - val_acc: 0.7619\n",
            "Epoch 75/100\n",
            "9777/9777 [==============================] - 288s 29ms/step - loss: 0.2861 - acc: 0.9007 - val_loss: 1.0349 - val_acc: 0.7671\n",
            "Epoch 76/100\n",
            "9777/9777 [==============================] - 285s 29ms/step - loss: 1.1318 - acc: 0.6137 - val_loss: 0.9741 - val_acc: 0.6025\n",
            "Epoch 77/100\n",
            "9777/9777 [==============================] - 282s 29ms/step - loss: 0.6702 - acc: 0.7198 - val_loss: 0.8490 - val_acc: 0.7170\n",
            "Epoch 78/100\n",
            "9777/9777 [==============================] - 287s 29ms/step - loss: 0.4131 - acc: 0.8291 - val_loss: 0.8416 - val_acc: 0.7645\n",
            "Epoch 79/100\n",
            "9777/9777 [==============================] - 284s 29ms/step - loss: 0.2841 - acc: 0.8943 - val_loss: 0.9369 - val_acc: 0.7659\n",
            "Epoch 80/100\n",
            "9777/9777 [==============================] - 281s 29ms/step - loss: 0.2780 - acc: 0.8995 - val_loss: 0.9837 - val_acc: 0.7683\n",
            "Epoch 81/100\n",
            "9777/9777 [==============================] - 278s 28ms/step - loss: 0.2765 - acc: 0.9021 - val_loss: 0.9872 - val_acc: 0.7707\n",
            "Epoch 82/100\n",
            "9777/9777 [==============================] - 282s 29ms/step - loss: 0.2409 - acc: 0.9112 - val_loss: 1.0317 - val_acc: 0.7922\n",
            "Epoch 83/100\n",
            "9777/9777 [==============================] - 283s 29ms/step - loss: 0.1780 - acc: 0.9357 - val_loss: 1.0859 - val_acc: 0.8031\n",
            "Epoch 84/100\n",
            "9777/9777 [==============================] - 281s 29ms/step - loss: 0.2302 - acc: 0.9249 - val_loss: 0.8799 - val_acc: 0.7719\n",
            "Epoch 85/100\n",
            "9777/9777 [==============================] - 289s 30ms/step - loss: 0.1931 - acc: 0.9326 - val_loss: 1.0803 - val_acc: 0.7962\n",
            "Epoch 86/100\n",
            "9777/9777 [==============================] - 286s 29ms/step - loss: 0.2124 - acc: 0.9252 - val_loss: 1.2846 - val_acc: 0.7492\n",
            "Epoch 87/100\n",
            "9777/9777 [==============================] - 289s 30ms/step - loss: 0.2255 - acc: 0.9164 - val_loss: 0.8733 - val_acc: 0.7998\n",
            "Epoch 88/100\n",
            "9777/9777 [==============================] - 289s 30ms/step - loss: 0.3212 - acc: 0.8916 - val_loss: 0.8515 - val_acc: 0.7227\n",
            "Epoch 89/100\n",
            "9777/9777 [==============================] - 287s 29ms/step - loss: 0.2655 - acc: 0.8969 - val_loss: 1.2879 - val_acc: 0.7941\n",
            "Epoch 90/100\n",
            "9777/9777 [==============================] - 304s 31ms/step - loss: 0.2274 - acc: 0.9205 - val_loss: 0.9603 - val_acc: 0.7946\n",
            "Epoch 91/100\n",
            "9777/9777 [==============================] - 292s 30ms/step - loss: 0.1461 - acc: 0.9493 - val_loss: 1.2251 - val_acc: 0.8172\n",
            "Epoch 92/100\n",
            "9777/9777 [==============================] - 287s 29ms/step - loss: 0.1510 - acc: 0.9480 - val_loss: 1.0135 - val_acc: 0.7989\n",
            "Epoch 93/100\n",
            "9777/9777 [==============================] - 281s 29ms/step - loss: 0.1482 - acc: 0.9476 - val_loss: 1.2181 - val_acc: 0.7981\n",
            "Epoch 94/100\n",
            "9777/9777 [==============================] - 280s 29ms/step - loss: 0.4152 - acc: 0.8937 - val_loss: 1.5624 - val_acc: 0.5631\n",
            "Epoch 95/100\n",
            "9777/9777 [==============================] - 283s 29ms/step - loss: 0.9037 - acc: 0.6239 - val_loss: 0.9390 - val_acc: 0.6292\n",
            "Epoch 96/100\n",
            "9777/9777 [==============================] - 288s 29ms/step - loss: 0.6119 - acc: 0.7318 - val_loss: 0.9005 - val_acc: 0.7175\n",
            "Epoch 97/100\n",
            "9777/9777 [==============================] - 287s 29ms/step - loss: 0.3781 - acc: 0.8445 - val_loss: 0.8905 - val_acc: 0.7733\n",
            "Epoch 98/100\n",
            "9777/9777 [==============================] - 290s 30ms/step - loss: 0.2625 - acc: 0.9013 - val_loss: 0.8880 - val_acc: 0.7958\n",
            "Epoch 99/100\n",
            "9777/9777 [==============================] - 292s 30ms/step - loss: 0.2059 - acc: 0.9251 - val_loss: 1.0702 - val_acc: 0.8008\n",
            "Epoch 100/100\n",
            "9777/9777 [==============================] - 288s 29ms/step - loss: 0.1725 - acc: 0.9405 - val_loss: 1.2455 - val_acc: 0.7938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#Train\\nhistory=model.fit_generator(trainGen,\\n                    steps_per_epoch=300,\\n                    epochs=100,\\n                    validation_data=(X_test, y_test) \\n                    validation_steps=60)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BITyR1WVjTJ8",
        "colab_type": "text"
      },
      "source": [
        "## 예측 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J94ty7hhxWRg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt3gP9Wcjaph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "99984072-00b9-444b-e1e7-544c38fc5c7d"
      },
      "source": [
        "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4191/4191 [==============================] - 36s 9ms/step\n",
            "정확도 : 0.7938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr5k-QIQhFe8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "ee449276-6a5b-4ab7-c99c-8d0426fc02b7"
      },
      "source": [
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  batch_size=3,\n",
        "                                                   class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1961 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrV2VTVUX3vg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c187b080-2a6f-430b-998b-f06d9402d01c"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Confution Matrix and Classification Report\n",
        "Y_pred = model.predict_generator(validationGen, 654)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(validationGen.classes, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[225  34  23 130]\n",
            " [ 85  17  11  37]\n",
            " [139  27  22  74]\n",
            " [597 138  84 318]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYgtHHn65wxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "ff2b8b0a-f588-492f-d0dd-9947a99aa14d"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "print(metrics.confusion_matrix(validationGen.classes, y_pred))\n",
        "\n",
        "print(metrics.classification_report(validationGen.classes, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[225  34  23 130]\n",
            " [ 85  17  11  37]\n",
            " [139  27  22  74]\n",
            " [597 138  84 318]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.55      0.31       412\n",
            "           1       0.08      0.11      0.09       150\n",
            "           2       0.16      0.08      0.11       262\n",
            "           3       0.57      0.28      0.38      1137\n",
            "\n",
            "    accuracy                           0.30      1961\n",
            "   macro avg       0.25      0.26      0.22      1961\n",
            "weighted avg       0.40      0.30      0.30      1961\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hohNPjQf7U5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrVWxw20K-tG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "43785573-294c-4156-f60f-6ad896df7e85"
      },
      "source": [
        "\n",
        "print('Classification Report')\n",
        "target_names = ['cancer', 'precancer', 'extra','normal']\n",
        "print(classification_report(validationGen.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b663c3058a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Classification Report'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cancer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'precancer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'extra'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'normal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidationGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlKVAN_d9Jl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "outputId": "9af16158-5cae-4ae5-aaee-ed4fbf580a4c"
      },
      "source": [
        "img_to_array(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[148.,  22.,  46.],\n",
              "        [148.,  22.,  46.],\n",
              "        [149.,  23.,  47.],\n",
              "        ...,\n",
              "        [210., 116., 168.],\n",
              "        [234., 142., 194.],\n",
              "        [233., 140., 193.]],\n",
              "\n",
              "       [[147.,  23.,  47.],\n",
              "        [148.,  24.,  48.],\n",
              "        [148.,  24.,  48.],\n",
              "        ...,\n",
              "        [214., 120., 173.],\n",
              "        [235., 142., 195.],\n",
              "        [230., 138., 192.]],\n",
              "\n",
              "       [[144.,  21.,  45.],\n",
              "        [145.,  21.,  45.],\n",
              "        [145.,  22.,  46.],\n",
              "        ...,\n",
              "        [218., 122., 178.],\n",
              "        [235., 141., 197.],\n",
              "        [228., 136., 192.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 80.,  24.,  33.],\n",
              "        [ 82.,  25.,  34.],\n",
              "        [ 81.,  24.,  33.],\n",
              "        ...,\n",
              "        [102.,  14.,  29.],\n",
              "        [104.,  16.,  30.],\n",
              "        [106.,  18.,  30.]],\n",
              "\n",
              "       [[ 81.,  25.,  34.],\n",
              "        [ 83.,  26.,  35.],\n",
              "        [ 80.,  24.,  32.],\n",
              "        ...,\n",
              "        [104.,  16.,  30.],\n",
              "        [107.,  19.,  33.],\n",
              "        [107.,  19.,  32.]],\n",
              "\n",
              "       [[ 82.,  25.,  34.],\n",
              "        [ 83.,  26.,  35.],\n",
              "        [ 81.,  23.,  32.],\n",
              "        ...,\n",
              "        [104.,  16.,  31.],\n",
              "        [107.,  19.,  33.],\n",
              "        [107.,  19.,  32.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKXhm3qC9Y8k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "e7541209-0661-418e-859a-91991b62d9c5"
      },
      "source": [
        "img"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAUdklEQVR4nKV627IkR3IjAI/IOt0kxaHJTLum3W/Yr91/lI0uM8Mhu8+pynBgHyKruinpbcuMxuo8WZlxcYcD8OD//af/A4AkSezP6wsAwElVleQVAINyKAoByTlmDzyYQpgQUPLTT3/69MNPQX2cax5vn3/4pfPWyIpdNPX2w+fH6vN8HOOo0qjxaQgiqGMeIO/vdxlHHXTe6piq95wPN4aa1O348U8//89//uc4IyTJPMcdAMn3E0iuf5PpziJGzbSrxhgDgW1q/yoMENw/PtoxRgfn6bX09onj7VaaC05pDoqihT7T5A0fH9aoOW81CmACBxSFohSAKiZr2aWpGmMSXPZom3sLnnPgNVw+94MESGrI6bRRSSJhjFpnpy2QCBMEAPpcaxmaHIf9OB+/xfzMHzIqwDFunwfrdvRtvn/9en//qBXdPktCcp5rtb9+3AuadUwyLAOOIyGqUXO+DU1Edo/zPEEQDCCSoiQAvEYOCYERUaqKncRAEtgOrIABAOaavFeHqAmBDpK1Hl++/vY43Y0cnz99fPl1zHnMeT4e59d3Po63+YnF1b3e3x3YkXgGsZPF4O4Hh1ij6lZjdmOtnPcep89rvQmG8HPVn5+jhtFhzzEBgrFNonslkVQSAgLENZO2x5xjDFTxWpbVbbvb/f77/f13ELjNiSBrZR5fVv38yz/WnL1WgxIhOT5NCkOSjqqyOI/j09vnmsdvf//yuD+G964n3KO7Yv6aEUnGBCOVJBUFxEQF3R2gRo392z3S2CHnGG+fPtXxtsBztde9Bt+O4+xlGzDBYqdXeo3i77/99uPnT0OffbZBQ4DW6lGqqjGPKdWs06vG+Omnnxv587/+u7sH6xvm7EAQ5dgJsdNJBCSNKlX1udpxVkIyXp1YkRDOUeSZqCrSOOZPP/98X/77ly/OuWwRq3vOMccUOYD74/E4Hy29HcL5aCmOw5q3WQJMZLmrV9XBUZN1e/t0vB1//td/e9w/pDHCF+ZciewNO3sLQJIliXtzDKB7MVeqA4RBkIljj0FJVQ2vBKPmGDd7vB0bBoDMoWPO8/3r+++/dTv2eZ6qu9ZjHpPkqNIs0mRJYPpcObGy3j/9+MMvv/x8e5tf3790nyWOs/sb2mzkIcFvV9otxNB5niQRkNk32QawnGKRcByiRi2kez16mRm3txvL4PHpbYyy13p8rHWm5rx97kf32VWT6+z7l8xSjTFrHrU6lAUjbTfH7OR+//jLr//x8LnWvX3aGkk/EXPnIdx+pjUhdUcpAm4lEYlQCCnb3Y3IhTnKyIoRd9Cq+zq/3D9uY9Tt7fbpRyQrrSEAvVwTx5hx1tmgDsUfX796PYJx++Gnf9SYb4iBBUCs5TU+vdXgn//8L3/7/dfH49F2t8bZ6wU4kpKdxIFRo7gLWYTEPr13lUMksGsgAqhqzEMFSvtRNart3778foLHpxoHqhQExP3sgIwCv7394B+67/eKP+739Xh8dM/P/eOf/lS3T8uOOcrFauaXX37+8acf/+XP//r1/nGeS1TQ47H6G24+sf+ajCGgUzBXAodBgNvUOA6G5+MBFqtYPJljzHkcLMGuWWCf738nzsrJPuePn8ZtdjIO6zZzP9d9vR3j0/H299/fP8bDE+f9ozg+jxr3e/CljdvnH9/ePjs88fD6+tPnX/420h+93r+OccgYr8jZIG47CSkFNEkXJkDYSZSQ2kmQBORxHFCdNtLBMMJEJbvJHMftVqys6vvMfBs3zsL4dH//eD8fZrxWuoloFIOZzHp7G+P+/tXvd87b7fZGn7b//S9//utf/23dP/7yH3+Nw8AnujPWcwc2CFEqVV4UKFhwsICkvW873WpJ0hxzTodeS5KAuO1myTYleBJJVq/7+5e8P752YsR2Hiv38+N++uMRn70IpFijaq31+Pq+jLq9dfrLl18/zmUGvb78+rfz/auNt88/xO1wnO5vCErCr5wGgQCNnhRIOAKT6CFQb1VV1QiA2gyEFNh20vOYos77ncDbLc6pxzDRsQWvRlttPay2gPPhXQqh1Wt5uUZx3e9f/3quqaof3/5h3N7UazCPPuXb2+cfwBoJQSBPAA3a3jmwM+Nc3bAkBaPKxTPNPmU1w4DBoCoAjAA2CGGWyMTnuYiqyrka6e4UGLJNp8QCnOWFWQPt5jlUU6ESnu4l+9OnTyV/vs2+f5TXp6NmZSpGjweMPHG/AcCwIgYbUYIUUQwkDKVk6eGVEwcOkRWmOyM0g3TM4P7xfsw55+xeH1/PYxZKnTipUSSzOu2Guns9TuRm0N1xj9vbMUqDKS138uGVY7zdKr/+/vs61+3zZ5/v726DI99IxLeS3EjSaOzw6ID2rELYjUevWYMowgyHIVZIAwBUZbvPs7udzBpbaDDpx2N119sbxNhwLy8GNasXz3USKJRtB1mNkCKS+/tX5va3c73f7x/3x2+//Q2q//G//vc//PDj8B/113ek6MoBwwREAoy7GILzOFz6WGfiN4451TToQYjqdJgSamgcoyQi3V0kJK8lEY7tdG+5lCaAAtp4nB/tYpFNFCME+XL+OsY8u9cyxxAn1n19aET6byZwIRBedW0zvO6IPGouAV5ul5Tiw6uY6hglKTALFI0YKZFBtwHOMbqNXBWwgc1/ncUN6HEvjHhg0LqqKEA+QG+WILJQ9EMZA//dDuA19MTfkf0gO67w8bUgkrfb7eFlR6lqzRo1aqhKauHRC4sgFbYjkiwS3ba92sveryFaYIOIigoU7IIjhCDaD7TtbK3T7d/+9lf8uAbw3+zAXn0gQXI9ZMvdIDmbzmNUlarvH5ZuIltD6syR4u02pE1DnnqDUsU5H8u247bXHg8BoIQ4DYhQDWkANIpQQITGQzSEIsYQwvv716LGfxk6n8I4gAFCgZ5bHqhAgIQIMej1WLBKqjkRt5GJ2wMWU0pAREJhIB2nsyV7oUqEynEyxfbarIqFoFdCt1AqbSbf1mZsVRoa67HO+/t48ojvg+eaB1kAIr42RJLiS6M5ydYJerRlNGJkjMH1GKlRGqzH6o911phVBcRX2FvgUCFBGm1xgJS2BF/tkEBAlDBil5SQ1DFmqaqEIaZHVf2n0Lf7CUJ7JgD4FJ5JCMTZl7Y9wVCddgcCyC8f70eNecwZixB1YxFMEhsOHCAr22wwbTtjDADnWqOqqi6jQEXym/cDjDEorrU2tx/iHyYAQqwEdttOQAnbxNrWFbPTJoCdAI1osGrOMUYVgI7f1+PhPuaYcxxDT2eJKUnIMpLNrglGhASxu3utGjXn3EtVNV5RYZtkEtu9liSA448uFkhW1ZZapOK0Oy9riFfKc78eDuKEjqZSamLFRGzDfbrfCIM5v45jFoVEZFEk4Y1vJOR4mUlQooRSbWtHSnt1L9IA17p/fKgKyaGj5hh74/4YQgZQVTud38970tsdoURCuBA2IexOus/10XqoRBFV21oi4491rtUrPHKbNQgWiTH2InCHIvnoNYExRkkRO2YVAifL67HOUCQbeO8eY4wxHuucxzGO4+0qW1uIgba7FylJ7q4aSiHZWUEqcRBpl3nLucIM6Q20bYlDaqDXamjUfJxnd88aoVRVc8TrXOdQMZGEEkcl6O4lSbINp9fq9B7AdqJsr7XO8+xkzDnxKlvAK9SeFzHrAHdKdIJ2qgo7rihQTxK0OW0M8potsc0iYMXqbL9jqPB4JIHj9HYQJuhNBLeoWnC2p7d6rSSzVBQESSQNtJ3HOTZWvoxdkgmq5lOwEyUAZlm9Pbmq6l0oJMTItfwIrlUCjJhRiRSBtg3UkzSmO4jIJOi9un55sgTO7ZmpbJ9rjTmrStQePSQCNUaQcZ7n94gpqarGGHsfRC4nwBgFTMDgBQqOIRIsOt5qwMzG1o7hfko4MdzudZi0e4OLne5uGslaCRKgqiQRWd1JTne4l3VjMLSzUwoRZ9zv929yDCA5xtj3JFHVNj4vtwEMncTb/1YBIJqbse7BO07HcdBZqFDa4JXsdwhAg7HXucYYJBP3nveCZZEAzhU4JDt2O06ImlOqqjKi0vA3SXl5KGudb29vkhI/lfLTtcDTsVORgRSHUu3qagFmMRzu7mybVE1cyhtspeii3IGDpCBSNQXRhN1cKalU3S2yVBTbl1XtpjKIWuukNOZU8hTxiZEqSZGSOMDqdiJJUo0hcbUjlg6QZzcdDLnbDESpJmd3n2vZeVAITm96mQIGWOTpCDqqHqgCVZ3BFMG4Y3SCqi0busSewJCTleX+6LORCBjfLz/IQUrq3mU453mebXAbnkUJUK5iQVWVVBoFv7B4u++S3m637l5rdTdLfN0AP3Oads3uklLtZJV3k0EklZfL2UCvBjejj2PbEm0PXCCIV4AE3e0qAXRWsPUgwNr0CnlBFmYVDHdfPgBAsrtfRX0vR3tn/+XW2HscMG25pBUvYm76ABbleKgQiOwVb1lwFSOQHKh4jRf8v7DfDrDDd6d8gJ3EuxVjMrF3D2u3C7o7yWUqPkFsV/Rdep4wspHmxc03dnqbNzF2H7GoIrEyawjcGyKwVBKxd0SqElVjXUbQ87/9AJW3d6ptmF6EdNM7I2utTnd6egp5jW9P44JC8jWHza92fwqBiltKPvcdyzF9OvHme2RyjAgXfZqlCWwZCDdb94VZNbjn9FygCyt6d5ripDR2O8yXtZvlXj67O0hJxzGF2rG+R+yns/TajbXWplJX7yFA8MqKBpppG4ASO5egtEvA7sq5Ytdm2qq2sWJ4jDl3fXR8lUnsPCQl24/zsbOGJEskP85Hd4OsUXOOOae2tM2mrdgaI0l3796jXhn2pIz0VbkDImGNxHsQu8yE0vbbBBsNb9+MoqGGCdEe8xjfxAqCwHE34q3iVhyCVS9P11UlkdJxO2qWxORa72uBST8/+4r+aB2QxAVLVyDtMG4ajnYjCOiGAGHbtW1YVCdKEMdwMMaoPFdlM1I2EnvDBLI5SEnP2zBH2aZYo2KvrMvkeeLB4/HYcPqaD/7rBAA/7082zIGByKrCpd0uJLnQZYsNc2OG3Bga4zi+0ekgSEshadMOmWW30wEAEWTW2jiy13gh9Leh7P89JRH38uuP7pOfGY8nDbyazZdJLj77vTGMFJFnr2XFux1MMckYY+z++iZkGzEo2rs3LnPbNwagKkpnLzwTf61VW54DrwbPmFPfxQ8Au//gHjy7QTvtCBYI7u45zBAQd38IMHYPOMyTDloiVQFHTW0gemkxADizcQNnVtGP1eeixILT26oBDdAOWNieFa4WU6/VQL9wLRG6xngBKxBf2HxNK9Ez2PDclg33hJjSkbFpZYBeMQ1A8jjXuTH7D+XmWUeTuM+qom77r2ut70OZZKn24RU8e1Mvp+NZvC6Ee/3k25+eX/QdI76K0jO1pV2NqnaWbIsQccxgvH98XCrh+blev8NRYpVIeJsiqTnwLEAXw2PBz+hHeHHvIq/B2XBekuMKHjzB6kVyvwmaXLJrf9m/8bOQb2J+6YNgvHjbK4SO43gJGgBjVPcWyi3pSd+TJ0UlNiY+68A+cQDsi/YOOlPCc+DfwOfqOe9ep7ivPBczm2Hq2Yvn7p5eD3Dsznicp56CMleHj5uQ7TQcdSX6hejS5mevOYsiuDXQs2Bf+3OVP4BV+5kbMZNvtfJSSg1ge1DQLjgOAWs7keYYurLj8tTQljLe7x/87iPJxOlu93YPQ85Zx9sbq7rbW7tsDYYduxH8fKoDX4sAAAZTQ9xnjJJ2e9sfr6AnQfR1WGqXXl3ZnE00LOrsFoDd1NL2gEVhbI/utbTHcQDYF58piH046ziOtZb30Ybv04aw7fUtmgFcFAssUFW8qBuyLhS94ufSUnuu32r2q6pucGf3AkuikqTMOQoitqh/+X4ibbdd0gvUrbS91nq96lLMgHf+kLqo+hMKARWT6IIdiFch09Pw2ZrhRVdD6LJgroaHL+MdG3tGlRHudBXZGBSI4WdwE2zHCMSMUYKk3bQ8z/NVIrYrtefXV30N4lcEX5mA4FuYRc824j51tMNmS4ONs16N2iwa6wlbmxBcNWF3GrYnDjbBdJzxn4B5w7zt4XHVB7K7d/5dIDvn9/oL3xVAPHcpT2Z6kQ74wg1frIMXS92QKrwKWVKs6EoJbOy6CKq/+QqiHSgDlxzBZjB8CpF+fr9C3O5uAt5M66VRkirxdYQuSDznMarydMoExt+g8w9zA7d5nsYrrnh54QFwnbu6NMYeD1Wl0oaD8chASFweipA4FLJOuzUqOkx5Dx1MVQOje6BJCJ0MekjQUPcZgJXIgagCKrD4YSfcbhBVtcFQKrJsg/BTP3yDY19sj4ABO4eqSlE5CGFmaN52tkOLZtydELBDxGZdpVD7RCOldjdAWxXtshMmFkWxpCpFYATgOtWpHQC7vbDjAt/5GFvAcM65U9zu5FXQLmoEsh1yn2+72vNj3o4A6O5Gr7XfmE0RCRNjF0JijKGqy1S0G723zdrg0LkGp6oKmeiCzuCCbpOVhOCV1b5cBG8eqO3fB0aTDK4Dcck2wGF34+kS2BKG5thOgE/Dy77GvXPFxMWynavMXfU/3R1bpVwgnyeof9MGF2ELnp5ixb7sgzDwtbrElVHXZKmS7SdGvZQbdlOzu3mZyBhrV1My2+PcYmZ3lQQLEn1xNe5kEnka3Q26maEg2eyx22tlPy0pAPa2sGkDyfNUybNCPSt2/AQT8lL74iuF8xRvO6i6ey9ogtFxwI3kzrUemzBdaoOUuLnWq7DgtfXRDtaNqknawVp4srQEDewDZt6aC9kFJ0mp9nnb7t4EnsCyt0Oxydwe0sau795iXYyRm7dmdYe5Op14nvzYvlr37kRtlWP3tQbPQvbSAORFZpNQm4Z5k6w9AFI21pMCd3e7AWh3IZL7eQY4bgc3od7KQKyqlyTavmJVkRp81mri2xFo8HkK7TWT/6/P9vSeuQEHAvz938MNLPuGbZ/o8jT5dBO/00Mvgfr/AJ0lL5H8VsTzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F10749B1710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofILoFWw61bu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "3111b77d-d67c-4877-ee62-65739dff8b8c"
      },
      "source": [
        "\n",
        "title = 'test'\n",
        "cmap=plt.cm.Greens\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(confusion_matrix, interpolation='nearest', cmap=cmap)  # , cmap=plt.cm.Greens\n",
        "plt.title(title, size=12)\n",
        "plt.colorbar(fraction=0.05, pad=0.05)\n",
        "tick_marks = np.arange(3, 3)\n",
        "plt.xticks(np.arange(4), (\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"))\n",
        "plt.yticks(np.arange(4), (\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"))\n",
        "\n",
        "fmt = 'd' \n",
        "thresh = 1\n",
        "for i in range(confusion_matrix.shape[0]):\n",
        "    for j in range(confusion_matrix.shape[1]):\n",
        "        plt.text(j, i, format(confusion_matrix[i, j], fmt),\n",
        "                 ha=\"center\", va=\"center\", \n",
        "                 color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")  #horizontalalignment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-5fba0b5b57a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGreens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# , cmap=plt.cm.Greens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    692\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m    693\u001b[0m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0;32m--> 694\u001b[0;31m                             \"float\".format(self._A.dtype))\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         if not (self._A.ndim == 2\n",
            "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOcIWHLgWMIS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQyZ1iPQOyu7",
        "colab_type": "text"
      },
      "source": [
        "## 모델을 이용해 새로운 이미지를 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiATXxnBj3oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "caltech_dir = \"/content/drive/My Drive/CTRC/train/multi_img_data/imgs_others_test\"\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "\n",
        "#테스트할 이미지를 변환할 소스\n",
        "X = []\n",
        "filenames = []\n",
        "files = glob.glob(caltech_dir+\"/*.*\")\n",
        "for i, f in enumerate(files):\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "    filenames.append(f)\n",
        "    X.append(data)\n",
        "\n",
        "X = np.array(X)\n",
        "X = X.reshape(X.shape[0], 64,64,3)\n",
        "#model = load_model( './model/multi_img_classification.model')\n",
        "\n",
        "\n",
        "#해당 이미지 predict\n",
        "prediction = model.predict(X)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "cnt = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n06sVosJj3lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atilE7sX0yGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "005e3b31-6604-4bb3-95ce-fb5de0086136"
      },
      "source": [
        "#저장된 모델구조와 모델 가중치를 불러옵니다.\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "#model=load_model('project.h5')\n",
        "\n",
        "\n",
        "\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  batch_size=3,\n",
        "                                                   class_mode='categorical')\n",
        "\n",
        "\n",
        "\n",
        "print(\"--Predict--\")\n",
        "\n",
        "output=model.predict_generator(trainGen,steps=220)\n",
        "\n",
        "np.set_printoptions(formatter={'float':lambda x:\"{0:0.3f}\".format(x)})\n",
        "\n",
        "print(trainGen.class_indices)\n",
        "\n",
        "print(output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11762 images belonging to 4 classes.\n",
            "--Predict--\n",
            "{'1. Cancer': 0, '2. Precancer': 1, '3. Extra': 2, '4. Normal': 3}\n",
            "[[0.053 0.111 0.344 0.492]\n",
            " [0.003 0.091 0.733 0.173]\n",
            " [0.000 0.000 0.000 1.000]\n",
            " ...\n",
            " [0.409 0.271 0.301 0.019]\n",
            " [0.697 0.233 0.070 0.000]\n",
            " [0.000 0.000 0.000 1.000]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBCM9KPDDJHO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b816ceba-6422-4f63-ee8a-ba7e6fcc1f8e"
      },
      "source": [
        "testGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  batch_size=3,\n",
        "                                                   class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1961 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpOWX84PoKT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "import numpy as np\n",
        "\n",
        "cls_index = ['1. Cancer', '2. Precancer','3. Extra','4. Normal']\n",
        "\n",
        "imgs = testGen.next()\n",
        "arr = imgs[0][0]\n",
        "\n",
        "\n",
        "img = array_to_img(arr).resize((128, 128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWR7EmL-oKQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8JCzkgVK3Iu",
        "colab_type": "text"
      },
      "source": [
        "## 성능 그래프 그리기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJH-rKjKWCuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "e4577f71-b977-458a-ed89-3591a12fa45f"
      },
      "source": [
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = np.arange(len(y_loss))\n",
        "\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1hkvrJRoeId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ad2ec9fa-56ff-4b93-a623-f109b4cf1bba"
      },
      "source": [
        "plt.clf() # 그래프를 초기화합니다.\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(x_len, acc, 'bo', label='Training acc')\n",
        "plt.plot(x_len, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic4vQQoUooLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "ef0d1b30-cef1-489c-dfa6-a4befdbed002"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "\n",
        "\n",
        "y = trainGen \n",
        "\n",
        "p = output\n",
        "\n",
        "\n",
        "\n",
        "accuracy = np.mean(np.equal(y,p))\n",
        "\n",
        "right = np.sum(y * p == 1)\n",
        "\n",
        "precision = right / np.sum(p)\n",
        "\n",
        "recall = right / np.sum(y)\n",
        "\n",
        "f1 = 2 * precision*recall/(precision+recall)\n",
        "\n",
        "\n",
        "\n",
        "print('accuracy',accuracy)\n",
        "\n",
        "print('precision', precision)\n",
        "\n",
        "print('recall', recall)\n",
        "\n",
        "print('f1', f1)\n",
        "\n",
        "\n",
        "\n",
        "# sklearn 을 이용하면 전부 계산해준다.\n",
        "\n",
        "print('accuracy', metrics.accuracy_score(y,p) )\n",
        "\n",
        "print('precision', metrics.precision_score(y,p) )\n",
        "\n",
        "print('recall', metrics.recall_score(y,p) )\n",
        "\n",
        "print('f1', metrics.f1_score(y,p) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-772769e8a738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                            interpolation=self.interpolation)\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2IJ8ebWrsIz",
        "colab_type": "text"
      },
      "source": [
        "## 혼동 행렬 그리기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzXtI-lwnshO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "008867aa-5d2e-4892-8228-9f42922e9868"
      },
      "source": [
        "testGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  batch_size=3,\n",
        "                                                   class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1961 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjuTFWDHo6Q9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output2=model.predict_generator(testGen,steps=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1eX23zDBbgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13c1cdc7-cf31-420c-bf05-62b7d69ba7ac"
      },
      "source": [
        "\n",
        "'''from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_test = [2, 0, 2, 2, 0, 1] #실제값\n",
        "y_pred = [0, 0, 2, 2, 0, 2] #예측값\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred) #혼동 행렬\n",
        "print(conf_matrix)'''\n",
        "\n",
        "'''\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_test = testGen #내 코드에서 수정\n",
        "y_pred = output2\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(trainGen, output) \n",
        "print(conf_matrix)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom sklearn.metrics import confusion_matrix\\n\\ny_test = testGen #내 코드에서 수정\\ny_pred = output2\\n\\n\\nconf_matrix = confusion_matrix(trainGen, output) \\nprint(conf_matrix)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTO0zXZ-rxPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b0e31bc2-285f-4f71-dd14-15238a36e9f2"
      },
      "source": [
        "'''\n",
        "title = 'test'\n",
        "cmap=plt.cm.Greens\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)  # , cmap=plt.cm.Greens\n",
        "plt.title(title, size=12)\n",
        "plt.colorbar(fraction=0.05, pad=0.05)\n",
        "tick_marks = np.arange(3, 3)\n",
        "plt.xticks(np.arange(4), (\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"))\n",
        "plt.yticks(np.arange(4), (\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"))\n",
        "\n",
        "fmt = 'd' \n",
        "thresh = 1\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        plt.text(j, i, format(conf_matrix[i, j], fmt),\n",
        "                 ha=\"center\", va=\"center\", \n",
        "                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")  #horizontalalignment'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntitle = \\'test\\'\\ncmap=plt.cm.Greens\\nplt.figure(figsize=(6, 6))\\nplt.imshow(conf_matrix, interpolation=\\'nearest\\', cmap=cmap)  # , cmap=plt.cm.Greens\\nplt.title(title, size=12)\\nplt.colorbar(fraction=0.05, pad=0.05)\\ntick_marks = np.arange(3, 3)\\nplt.xticks(np.arange(4), (\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"))\\nplt.yticks(np.arange(4), (\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"))\\n\\nfmt = \\'d\\' \\nthresh = 1\\nfor i in range(conf_matrix.shape[0]):\\n    for j in range(conf_matrix.shape[1]):\\n        plt.text(j, i, format(conf_matrix[i, j], fmt),\\n                 ha=\"center\", va=\"center\", \\n                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")  #horizontalalignment'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph43TapLdCBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Confusion matrix\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(nb_classes))\n",
        "    plt.xticks(tick_marks, nb_classes, rotation=45)\n",
        "    plt.yticks(tick_marks, nb_classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(X_train)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(trainGen,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(10))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt='d')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDVCDIfF-XI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ_CKCd8-htm",
        "colab_type": "text"
      },
      "source": [
        "# 한승현 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25tD9c67-jDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def load_train(train_path, image_size, classes):\n",
        "    images = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    cls = []\n",
        "\n",
        "    print('Reading training images')\n",
        "    for fld in classes:\n",
        "        index = classes.index(fld)\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\n",
        "        path = os.path.join(train_path, fld, '*g')\n",
        "        files = glob.glob(path)\n",
        "        for fl in files:\n",
        "            image = cv2.imread(fl)\n",
        "            image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\n",
        "            images.append(image)\n",
        "            label = np.zeros(len(classes))\n",
        "            label[index] = 1.0\n",
        "            labels.append(label)\n",
        "            flbase = os.path.basename(fl)\n",
        "            ids.append(flbase)\n",
        "            cls.append(fld)\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    ids = np.array(ids)\n",
        "    cls = np.array(cls)\n",
        "  \n",
        "\n",
        "    return images, labels, ids, cls\n",
        "\n",
        "\n",
        "def load_test(test_path, image_size):\n",
        "    path = os.path.join(test_path, '*g')\n",
        "    files = sorted(glob.glob(path))\n",
        "\n",
        "    X_test = []\n",
        "    X_test_id = []\n",
        "    print(\"Reading test images\")\n",
        "    for fl in files:\n",
        "        \n",
        "        img = cv2.imread(fl)\n",
        "        img = cv2.resize(img, (image_size, image_size), cv2.INTER_LINEAR)\n",
        "        X_test.append(img)\n",
        "        X_test_id.append(flbase)\n",
        "        flbase = os.path.basename(fl)\n",
        "\n",
        "    X_test = np.array(X_test, dtype=np.uint8)\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_test = X_test / 255\n",
        "    X_test_id = np.array(X_test_id)\n",
        "\n",
        "    return X_test, X_test_id\n",
        "\n",
        "\n",
        "class DataSet(object):\n",
        "\n",
        "    def __init__(self, images, labels, ids, cls):\n",
        "        \"\"\"Construct a DataSet. one_hot arg is used only if fake_data is true.\"\"\"\n",
        "\n",
        "        self._num_examples = images.shape[0]\n",
        "\n",
        "        # Convert shape from [num examples, rows, columns, depth]\n",
        "        # to [num examples, rows*columns] (assuming depth == 1)\n",
        "        # Convert from [0, 255] -> [0.0, 1.0].\n",
        "\n",
        "        images = images.astype(np.float32)\n",
        "        images = np.multiply(images, 1.0 / 255.0)\n",
        "\n",
        "        self._images = images\n",
        "        self._labels = labels\n",
        "        self._ids = ids\n",
        "        self._cls = cls\n",
        "        self._epochs_completed = 0\n",
        "        self._index_in_epoch = 0\n",
        "\n",
        "    @property\n",
        "    def images(self):\n",
        "        return self._images\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._labels\n",
        "\n",
        "    @property\n",
        "    def ids(self):\n",
        "        return self._ids\n",
        "\n",
        "    @property\n",
        "    def cls(self):\n",
        "        return self._cls\n",
        "\n",
        "    @property\n",
        "    def num_examples(self):\n",
        "        return self._num_examples\n",
        "\n",
        "    @property\n",
        "    def epochs_completed(self):\n",
        "        return self._epochs_completed\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
        "        start = self._index_in_epoch\n",
        "        self._index_in_epoch += batch_size\n",
        "\n",
        "        if self._index_in_epoch > self._num_examples:\n",
        "            # Finished epoch\n",
        "            self._epochs_completed += 1\n",
        "\n",
        "            # # Shuffle the data (maybe)\n",
        "            # perm = np.arange(self._num_examples)\n",
        "            # np.random.shuffle(perm)\n",
        "            # self._images = self._images[perm]\n",
        "            # self._labels = self._labels[perm]\n",
        "            # Start next epoch\n",
        "\n",
        "            start = 0\n",
        "            self._index_in_epoch = batch_size\n",
        "            assert batch_size <= self._num_examples\n",
        "        end = self._index_in_epoch\n",
        "\n",
        "        return self._images[start:end], self._labels[start:end], self._ids[start:end], self._cls[start:end]\n",
        "\n",
        "\n",
        "def read_train_sets(train_path, image_size, classes, validation_size=0):\n",
        "    class DataSets(object):\n",
        "        pass\n",
        "\n",
        "    data_sets = DataSets()\n",
        "\n",
        "    images, labels, ids, cls = load_train(train_path, image_size, classes)\n",
        "    images, labels, ids, cls = shuffle(images, labels, ids, cls)  # shuffle the data\n",
        "\n",
        "    if isinstance(validation_size, float):\n",
        "        validation_size = int(validation_size * images.shape[0])\n",
        "\n",
        "    validation_images = images[:validation_size]\n",
        "    validation_labels = labels[:validation_size]\n",
        "    validation_ids = ids[:validation_size]\n",
        "    validation_cls = cls[:validation_size]\n",
        "\n",
        "    train_images = images[validation_size:]\n",
        "    train_labels = labels[validation_size:]\n",
        "    train_ids = ids[validation_size:]\n",
        "    train_cls = cls[validation_size:]\n",
        "\n",
        "    data_sets.train = DataSet(train_images, train_labels, train_ids, train_cls)\n",
        "    data_sets.valid = DataSet(validation_images, validation_labels, validation_ids, validation_cls)\n",
        "\n",
        "    return data_sets\n",
        "\n",
        "\n",
        "def read_test_sets(test_path, image_size):\n",
        "    images, ids = load_test(test_path, image_size)\n",
        "    return images, ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wayI0jn1-v5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "import cv2\n",
        "import dataset\n",
        "import os\n",
        "import keras\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from datetime import timedelta\n",
        "import seaborn as sn\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "filter_size1 = 5\n",
        "num_filters1 = 16\n",
        "\n",
        "filter_size2 = 5\n",
        "num_filters2 = 16\n",
        "\n",
        "filter_size3 = 5\n",
        "num_filters3 = 32\n",
        "\n",
        "filter_size4 = 5\n",
        "num_filters4 = 32\n",
        "\n",
        "filter_size5 = 5\n",
        "num_filters5 = 32\n",
        "\n",
        "filter_size6 = 7\n",
        "num_filters6 = 64\n",
        "\n",
        "filter_size7 = 7\n",
        "num_filters7 = 64\n",
        "\n",
        "# Fully-connected layer.\n",
        "fc_size = 256             # Number of neurons in fully-connected layer.\n",
        "\n",
        "# Number of color channels for the images: 1 channel for gray-scale.\n",
        "num_channels = 3\n",
        "\n",
        "# image dimensions (only squares for now)\n",
        "img_size = 128\n",
        "\n",
        "# Size of image when flattened to a single dimension\n",
        "img_size_flat = img_size * img_size * num_channels\n",
        "\n",
        "# Tuple with height and width of images used to reshape arrays.\n",
        "img_shape = (img_size, img_size)\n",
        "\n",
        "# class info\n",
        "classes = ['1.Cancer', '2.Precancer', '3.Inflammatory', '4.Normal']\n",
        "num_classes = len(classes)\n",
        "\n",
        "# batch size\n",
        "batch_size = 32\n",
        "\n",
        "# validation split\n",
        "validation_size = .3\n",
        "\n",
        "# how long to wait after validation loss stops improving before terminating training\n",
        "early_stopping = None  # use None if you don't want to implement early stoping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8RFmIJU-y1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = '/content/drive/My Drive/train'\n",
        "test_path = '/content/drive/My Drive/test'\n",
        "checkpoint_dir = '/content/drive/My Drive/model'\n",
        "\n",
        "data = read_train_sets(train_path, img_size, classes, validation_size=validation_size)\n",
        "test_images, test_ids = read_test_sets(test_path, img_size)\n",
        "\n",
        "\n",
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
        "print(\"- Test-set:\\t\\t{}\".format(len(test_images)))\n",
        "print(\"- Validation-set:\\t{}\".format(len(data.valid.labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1hv2iDG-zhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images(images, cls_true, cls_pred=None):\n",
        "    \n",
        "    if len(images) == 0:\n",
        "        print(\"no images to show\")\n",
        "        return \n",
        "    else:\n",
        "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
        "        \n",
        "    if cls_pred is not None:\n",
        "        images, cls_true, cls_pred  = zip(*[(images[i], cls_true[i], cls_pred[i]) for i in random_indices])\n",
        "    else:\n",
        "        images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
        "\n",
        "    print(cls_pred)\n",
        "    \n",
        "    # Create figure with 3x3 sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Plot image.\n",
        "        ax.imshow(images[i].reshape((img_size, img_size, num_channels)))\n",
        "\n",
        "        # Show true and predicted classes.\n",
        "        if cls_pred is None:\n",
        "            xlabel = \"True: {0}\".format(cls_true[i])\n",
        "        else:\n",
        "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
        "\n",
        "        # Show the classes as the label on the x-axis.\n",
        "        ax.set_xlabel(xlabel)\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()\n",
        "\n",
        "images, cls_true  = data.train.images, data.train.cls\n",
        "\n",
        "# Plot the images and labels using our helper-function above.\n",
        "plot_images(images=images, cls_true=cls_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGme578p-1Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_weights(shape):\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
        "\n",
        "def new_biases(length):\n",
        "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
        "\n",
        "def new_conv_layer(input,              \n",
        "                   num_input_channels, \n",
        "                   filter_size,       \n",
        "                   num_filters,       \n",
        "                   use_pooling=True): \n",
        "\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
        "\n",
        "    weights = new_weights(shape=shape)\n",
        "\n",
        "    biases = new_biases(length=num_filters)\n",
        "\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                         filter=weights,\n",
        "                         strides=[1, 1, 1, 1],\n",
        "                         padding='SAME')\n",
        "\n",
        "    layer += biases\n",
        "\n",
        "    if use_pooling:\n",
        "\n",
        "        layer = tf.nn.max_pool(value=layer,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME')\n",
        "\n",
        "    layer = tf.nn.relu(layer)\n",
        "\n",
        "\n",
        "    return layer, weights\n",
        "\n",
        "def new_conv_layer_drop(input,              \n",
        "                   num_input_channels, \n",
        "                   filter_size,       \n",
        "                   num_filters,       \n",
        "                   use_pooling=True): \n",
        "\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
        "\n",
        "    weights = new_weights(shape=shape)\n",
        "\n",
        "    biases = new_biases(length=num_filters)\n",
        "\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                         filter=weights,\n",
        "                         strides=[1, 1, 1, 1],\n",
        "                         padding='SAME')\n",
        "\n",
        "    layer += biases\n",
        "\n",
        "    if use_pooling:\n",
        "\n",
        "        layer = tf.nn.max_pool(value=layer,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME')\n",
        "\n",
        "    layer = tf.nn.relu(layer)\n",
        "    layer = tf.nn.dropout(layer, keep_prob=keep_prob)\n",
        "    \n",
        "    return layer, weights\n",
        "\n",
        "def flatten_layer(layer):\n",
        "\n",
        "    layer_shape = layer.get_shape()\n",
        "\n",
        "    num_features = layer_shape[1:8].num_elements()\n",
        "    \n",
        "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
        "\n",
        "    return layer_flat, num_features\n",
        "\n",
        "  \n",
        "def new_fc_layer(input,        \n",
        "                 num_inputs,    \n",
        "                 num_outputs,  \n",
        "                 use_relu=True):\n",
        "\n",
        "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
        "    biases = new_biases(length=num_outputs)\n",
        "\n",
        "    layer = tf.matmul(input, weights) + biases\n",
        "    \n",
        "    if use_relu:\n",
        "        layer = tf.nn.relu(layer)\n",
        "\n",
        "    return layer\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
        "\n",
        "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
        "\n",
        "y_true = tf.compat.v1.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
        "\n",
        "y_true_cls = tf.argmax(y_true, axis=1)\n",
        "\n",
        "keep_prob = tf.compat.v1.placeholder(tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T17VZN--3X0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_conv1, weights_conv1 = \\\n",
        "    new_conv_layer_drop(input=x_image,\n",
        "                   num_input_channels=num_channels,\n",
        "                   filter_size=filter_size1,\n",
        "                   num_filters=num_filters1,\n",
        "                   use_pooling=True)\n",
        "     \n",
        "\n",
        "layer_conv2, weights_conv2 = \\\n",
        "    new_conv_layer(input=layer_conv1,\n",
        "                   num_input_channels=num_filters1,\n",
        "                   filter_size=filter_size2,\n",
        "                   num_filters=num_filters2,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv3, weights_conv3 = \\\n",
        "    new_conv_layer_drop(input=layer_conv2,\n",
        "                   num_input_channels=num_filters2,\n",
        "                   filter_size=filter_size3,\n",
        "                   num_filters=num_filters3,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv4, weights_conv4 = \\\n",
        "    new_conv_layer(input=layer_conv3,\n",
        "                   num_input_channels=num_filters3,\n",
        "                   filter_size=filter_size4,\n",
        "                   num_filters=num_filters4,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv5, weights_conv5 = \\\n",
        "    new_conv_layer_drop(input=layer_conv4,\n",
        "                   num_input_channels=num_filters4,\n",
        "                   filter_size=filter_size5,\n",
        "                   num_filters=num_filters5,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv6, weights_conv6 = \\\n",
        "    new_conv_layer(input=layer_conv5,\n",
        "                   num_input_channels=num_filters5,\n",
        "                   filter_size=filter_size6,\n",
        "                   num_filters=num_filters6,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv7, weights_conv7 = \\\n",
        "    new_conv_layer_drop(input=layer_conv6,\n",
        "                   num_input_channels=num_filters6,\n",
        "                   filter_size=filter_size7,\n",
        "                   num_filters=num_filters7,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_flat, num_features = flatten_layer(layer_conv7)\n",
        "\n",
        "layer_fc1 = new_fc_layer(input=layer_flat,\n",
        "                         num_inputs=num_features,\n",
        "                         num_outputs=fc_size,\n",
        "                         use_relu=True)\n",
        "\n",
        "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
        "                         num_inputs=fc_size,\n",
        "                         num_outputs=128,\n",
        "                         use_relu=False)\n",
        "\n",
        "layer_fc3 = new_fc_layer(input=layer_fc2,\n",
        "                         num_inputs=128,\n",
        "                         num_outputs=64,\n",
        "                         use_relu=False)\n",
        "\n",
        "layer_fc4 = new_fc_layer(input=layer_fc3,\n",
        "                         num_inputs=64,\n",
        "                         num_outputs=32,\n",
        "                         use_relu=False)\n",
        "\n",
        "layer_fc5 = new_fc_layer(input=layer_fc4,\n",
        "                         num_inputs=32,\n",
        "                         num_outputs=16,\n",
        "                         use_relu=False)\n",
        "\n",
        "layer_fc6 = new_fc_layer(input=layer_fc5,\n",
        "                         num_inputs=16,\n",
        "                         num_outputs=num_classes,                        \n",
        "                         use_relu=False)\n",
        "\n",
        "y_pred = tf.nn.softmax(layer_fc6)\n",
        "\n",
        "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "loss_func = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc6, labels=y_true)\n",
        "\n",
        "cost = tf.reduce_mean(loss_func)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
        "\n",
        "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X7ZPQCz-58b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()\n",
        "\n",
        "session.run(tf.global_variables_initializer())\n",
        "\n",
        "train_batch_size = batch_size\n",
        "\n",
        "def print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
        "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
        "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
        "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%}, Validation Loss: {3:.3f}\"\n",
        "    print(msg.format(epoch + 1, acc, val_acc, val_loss))\n",
        "    print(print_validation_accuracy(show_confusion_matrix=True))\n",
        "\n",
        "total_iterations = 0\n",
        "\n",
        "def optimize(num_iterations):\n",
        "    global total_iterations\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience = 0\n",
        "\n",
        "    for i in range(total_iterations,\n",
        "                   total_iterations + num_iterations):\n",
        "\n",
        "        \n",
        "\n",
        "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
        "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
        "\n",
        "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
        "        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\n",
        "\n",
        "        feed_dict_train = {x: x_batch,\n",
        "                           y_true: y_true_batch, keep_prob: 0.7}\n",
        "        \n",
        "        feed_dict_validate = {x: x_valid_batch,\n",
        "                              y_true: y_valid_batch, keep_prob : 0.7}\n",
        "\n",
        "        session.run(optimizer, feed_dict=feed_dict_train)\n",
        "        \n",
        "        if i % int(data.train.num_examples/batch_size) == 0: \n",
        "            val_loss = session.run(cost, feed_dict=feed_dict_validate)\n",
        "            epoch = int(i / int(data.train.num_examples/batch_size))\n",
        "\n",
        "            print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss)\n",
        "            \n",
        "            if early_stopping:    \n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience = 0\n",
        "                else:\n",
        "                    patience += 1\n",
        "\n",
        "                if patience == early_stopping:\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "    total_iterations += num_iterations\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    time_dif = end_time - start_time\n",
        "\n",
        "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
        "\n",
        "def print_validation_accuracy(show_example_errors=False, show_confusion_matrix=False):\n",
        "\n",
        "    num_test = len(data.valid.images)\n",
        "\n",
        "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < num_test:\n",
        " \n",
        "        j = min(i + batch_size, num_test)\n",
        "\n",
        "        images = data.valid.images[i:j, :].reshape(-1, img_size_flat)\n",
        "        \n",
        "        labels = data.valid.labels[i:j, :]\n",
        "\n",
        "        feed_dict = {x: images, y_true: labels, keep_prob: 0.7}\n",
        "\n",
        "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
        "\n",
        "        i = j\n",
        "\n",
        "\n",
        "    cls_true = np.array(data.valid.cls)\n",
        "    cls_pred = np.array([classes[x] for x in cls_pred]) \n",
        "\n",
        "    correct = (cls_true == cls_pred)\n",
        "\n",
        "    correct_sum = correct.sum()\n",
        "\n",
        "    acc = float(correct_sum) / num_test\n",
        "\n",
        "    msg = \"Accuracy on Set: {0:.1%} ({1} / {2})\"\n",
        "    print(msg.format(acc, correct_sum, num_test))\n",
        "\n",
        "    if show_example_errors:\n",
        "        print(\"Example errors:\")\n",
        "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
        "\n",
        "    if show_confusion_matrix:\n",
        "        print(\"Confusion Matrix:\")\n",
        "        plot_confusion_matrix(cls_pred=cls_pred)\n",
        "\n",
        "def plot_example_errors(cls_pred, correct):\n",
        "\n",
        "    incorrect = (correct == False)\n",
        "\n",
        "    images = data.valid.images[incorrect]\n",
        "\n",
        "    cls_pred = cls_pred[incorrect]\n",
        "\n",
        "    cls_true = data.valid.cls[incorrect]\n",
        "\n",
        "    plot_images(images=images, cls_true=cls_true, cls_pred=cls_pred)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cls_pred):\n",
        "    \n",
        "    cls_true = data.valid.cls\n",
        "\n",
        "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\n",
        "    \n",
        "    print(cm)\n",
        "    \n",
        "    plt.matshow(cm, cmap=plt.cm.Wistia_r)\n",
        "\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(num_classes)\n",
        "    plt.xticks(tick_marks, range(num_classes))\n",
        "    plt.yticks(tick_marks, range(num_classes))\n",
        "\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    for i in range(0,4):\n",
        "      for j in range(0,4):\n",
        "        plt.text(j,i,str(cm[i][j]))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"1.Cancer의 정밀도 : \", (cm[0,0]/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]))*100, \"%\")\n",
        "    print(\"1.Cancer의 재현율 : \", (cm[0,0]/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]))*100, \"%\")\n",
        "    print(\"2.Precancer의 정밀도 : \", (cm[1,1]/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]))*100, \"%\")\n",
        "    print(\"2.Precancer의 재현율 : \", (cm[1,1]/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]))*100, \"%\")\n",
        "    print(\"3.Inflammatory의 정밀도 : \", (cm[2,2]/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]))*100, \"%\")\n",
        "    print(\"3.Inflammatory의 재현율 : \", (cm[2,2]/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]))*100, \"%\")\n",
        "    print(\"4.Normal의 정밀도 : \", (cm[3,3]/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]))*100, \"%\")\n",
        "    print(\"4.Normal의 재현율 : \", (cm[3,3]/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]))*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFbWnLMf--Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimize(num_iterations=1000000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmnWc_a0--i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cls_pred):\n",
        "    \n",
        "    cls_true = data.valid.cls\n",
        "\n",
        "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\n",
        "    \n",
        "    print(cm)\n",
        "    \n",
        "    plt.matshow(cm, cmap=plt.cm.Wistia_r)\n",
        "\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(num_classes)\n",
        "    plt.xticks(tick_marks, range(num_classes))\n",
        "    plt.yticks(tick_marks, range(num_classes))\n",
        "\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    for i in range(0,4):\n",
        "      for j in range(0,4):\n",
        "        plt.text(j,i,str(cm[i][j]))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"1.Cancer의 정밀도 : \", round((cm[0,0]/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]))*100,2), \"%\")\n",
        "    print(\"1.Cancer의 재현율 : \", round((cm[0,0]/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]))*100,2), \"%\")\n",
        "    print(\"2.Precancer의 정밀도 : \", round((cm[1,1]/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]))*100,2), \"%\")\n",
        "    print(\"2.Precancer의 재현율 : \", round((cm[1,1]/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]))*100,2), \"%\")\n",
        "    print(\"3.Inflammatory의 정밀도 : \", round((cm[2,2]/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]))*100,2), \"%\")\n",
        "    print(\"3.Inflammatory의 재현율 : \", round((cm[2,2]/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]))*100,2), \"%\")\n",
        "    print(\"4.Normal의 정밀도 : \", round((cm[3,3]/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]))*100,2), \"%\")\n",
        "    print(\"4.Normal의 재현율 : \", round((cm[3,3]/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]))*100,2), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fFRMmXH_CHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_validation_accuracy(show_example_errors=True, show_confusion_matrix=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeqJxJ3M_Fx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv('C:\\\\ml\\\\molecular_activity.csv')\n",
        "\n",
        "properties = list(df.columns.values)\n",
        "properties.remove('Activity')\n",
        "\n",
        "X = df[properties]\n",
        "y = df['Activity']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(4,)),\n",
        "    keras.layers.Dense(4, activation=tf.nn.relu),\n",
        "\tkeras.layers.Dense(4, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=34, batch_size=1, validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSuuXXvN_Ofc",
        "colab_type": "text"
      },
      "source": [
        "## 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5wvvUhE_cXn",
        "colab_type": "text"
      },
      "source": [
        "print_validation_accuracy(show_example_errors=True, show_confusion_matrix=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JWwlVXz_PjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_cancer = cv2.imread('/content/drive/My Drive/train/Cancer/can_129.jpg')\n",
        "test_cancer = cv2.resize(test_cancer, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "\n",
        "preview_cancer = plt.imshow(test_cancer.reshape(img_size, img_size, num_channels))\n",
        "\n",
        "def sample_prediction(test_im):\n",
        "    \n",
        "    feed_dict_test = {\n",
        "        x: test_im.reshape(1, img_size_flat),\n",
        "        y_true: np.array([[3, 2, 1, 0]])\n",
        "    }\n",
        "\n",
        "    test_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
        "    return classes[test_pred[0]]\n",
        "\n",
        "print(\"Predicted class for test_cancer: {}\".format(sample_prediction(test_cancer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiMBNGoZ_gvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inflammatory = cv2.imread('/content/drive/My Drive/train/Inflammatory/inf_139.jpg')\n",
        "test_inflammatory = cv2.resize(test_inflammatory, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "\n",
        "preview_inflammatory = plt.imshow(test_inflammatory.reshape(img_size, img_size, num_channels))\n",
        "\n",
        "def sample_prediction(test_im):\n",
        "    \n",
        "    feed_dict_test = {\n",
        "        x: test_im.reshape(1, img_size_flat),\n",
        "        y_true: np.array([[3, 2, 1, 0]])\n",
        "    }\n",
        "\n",
        "    test_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
        "    return classes[test_pred[0]]\n",
        "\n",
        "print(\"Predicted class for test_inflammatory: {}\".format(sample_prediction(test_inflammatory)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgW_OQgd_hs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_normal = cv2.imread('/content/drive/My Drive/train/Normal/nor_15.jpg')\n",
        "test_normal = cv2.resize(test_normal, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "\n",
        "preview_normal = plt.imshow(test_normal.reshape(img_size, img_size, num_channels))\n",
        "\n",
        "def sample_prediction(test_im):\n",
        "    \n",
        "    feed_dict_test = {\n",
        "        x: test_im.reshape(1, img_size_flat),\n",
        "        y_true: np.array([[3, 2, 1, 0]])\n",
        "    }\n",
        "\n",
        "    test_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
        "    return classes[test_pred[0]]\n",
        "\n",
        "print(\"Predicted class for test_normal: {}\".format(sample_prediction(test_normal)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4bhF0pL_ji1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_precancer = cv2.imread('/content/drive/My Drive/train/Precancer/precan_29.jpg')\n",
        "test_precancer = cv2.resize(test_precancer, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "\n",
        "preview_precancer = plt.imshow(test_precancer.reshape(img_size, img_size, num_channels))\n",
        "\n",
        "def sample_prediction(test_im):\n",
        "    \n",
        "    feed_dict_test = {\n",
        "        x: test_im.reshape(1, img_size_flat),\n",
        "        y_true: np.array([[3, 2, 1, 0]])\n",
        "    }\n",
        "\n",
        "    test_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
        "    return classes[test_pred[0]]\n",
        "\n",
        "print(\"Predicted class for test_precancer: {}\".format(sample_prediction(test_precancer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtYdhIJS_l3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_cancer = cv2.imread('/content/drive/My Drive/train/1.Cancer/can_163.jpg')\n",
        "test_cancer = cv2.resize(test_cancer, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "\n",
        "preview_cancer = plt.imshow(test_cancer.reshape(img_size, img_size, num_channels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGO-1NSu_pKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_precancer = cv2.imread('/content/drive/My Drive/train/2.Precancer/precan_125.jpg')\n",
        "test_precancer = cv2.resize(test_precancer, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "\n",
        "preview_precancer = plt.imshow(test_precancer.reshape(img_size, img_size, num_channels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlIyKiYQ_rmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inflammatory = cv2.imread('/content/drive/My Drive/train/3.Inflammatory/inf_146.jpg')\n",
        "test_inflammatory = cv2.resize(test_inflammatory, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "\n",
        "preview_inflammatory = plt.imshow(test_inflammatory.reshape(img_size, img_size, num_channels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-pDugCg_sgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_normal = cv2.imread('/content/drive/My Drive/train/4.Normal/nor_135.jpg')\n",
        "test_normal = cv2.resize(test_normal, (img_size, img_size), cv2.INTER_LINEAR) / 255\n",
        "\n",
        "preview_normal = plt.imshow(test_normal.reshape(img_size, img_size, num_channels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu6CrhDq_u4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_prediction(test_im):\n",
        "    \n",
        "    feed_dict_test = {\n",
        "        x: test_im.reshape(1, img_size_flat),\n",
        "        y_true: np.array([[3, 2, 1, 0]])\n",
        "    }\n",
        "\n",
        "    test_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
        "    return classes[test_pred[0]]\n",
        "\n",
        "print(\"Predicted class for test_cancer: {}\".format(sample_prediction(test_cancer)))\n",
        "print(\"Predicted class for test_precancer: {}\".format(sample_prediction(test_precancer)))\n",
        "print(\"Predicted class for test_inflammatory: {}\".format(sample_prediction(test_inflammatory)))\n",
        "print(\"Predicted class for test_normal: {}\".format(sample_prediction(test_normal)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pZpWDql_wRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_conv_weights(weights, input_channel=0):\n",
        "    \n",
        "    w = session.run(weights)\n",
        "\n",
        "    w_min = np.min(w)\n",
        "    w_max = np.max(w)\n",
        "\n",
        "    num_filters = w.shape[3]\n",
        "\n",
        "    num_grids = math.ceil(math.sqrt(num_filters))\n",
        "    \n",
        "    fig, axes = plt.subplots(num_grids, num_grids)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i<num_filters:\n",
        "\n",
        "            img = w[:, :, input_channel, i]\n",
        "\n",
        "            ax.imshow(img, vmin=w_min, vmax=w_max,\n",
        "                      interpolation='nearest', cmap='seismic')\n",
        "        \n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def plot_conv_layer(layer, image):\n",
        "    \n",
        "    image = image.reshape(img_size_flat)\n",
        "\n",
        "    feed_dict = {x: [image]}\n",
        "\n",
        "    values = session.run(layer, feed_dict=feed_dict)\n",
        "\n",
        "    num_filters = values.shape[3]\n",
        "\n",
        "    num_grids = math.ceil(math.sqrt(num_filters))\n",
        "    \n",
        "    fig, axes = plt.subplots(num_grids, num_grids)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i<num_filters:\n",
        "            img = values[0, :, :, i]\n",
        "\n",
        "            ax.imshow(img, interpolation='nearest', cmap='binary')\n",
        "        \n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def plot_image(image):\n",
        "    plt.imshow(image.reshape(img_size, img_size, num_channels),\n",
        "               interpolation='nearest')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REwOZuiH_z5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_image(image):\n",
        "    plt.imshow(image.reshape(img_size, img_size, num_channels),\n",
        "               interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "image1 = test_images\n",
        "plot_image(image1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxLZxuiE_5zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image2 = test_images[13]\n",
        "plot_image(image2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz_ZdIUW_7my",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image3 = test_images[13]\n",
        "plot_image(image3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GebSjqk-_9nO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image4 = test_images[13]\n",
        "plot_image(image4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKXn1UAuAAEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conv_weights(weights=weights_conv1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uYFjK13ACBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conv_layer(layer=layer_conv1, image=image1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz3ZGYzzAEbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conv_layer(layer=layer_conv1, image=image2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBNM-227AGf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conv_weights(weights=weights_conv2, input_channel=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJJL7vQrAGd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conv_weights(weights=weights_conv2, input_channel=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC-iSgXyAGM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conv_layer(layer=layer_conv2, image=image1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzRyQAYKAGJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_conv_layer(layer=layer_conv2, image=image2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygldiJOfAMwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJFOjfpfAPP4",
        "colab_type": "text"
      },
      "source": [
        "# 클러스터링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXmjrKnaARpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMsMzzSNC-qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"클러스터 레이블 확인 : \\n {}\".format(kmeans.y_train))\n",
        "# 클러스터 레이블 확인 :"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EPDSBkKDAOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.array_equal(kmeans.labels_,kmeans.predict(X)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}