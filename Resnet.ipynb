{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1mRkK12sxaMvjUPsyAvUhZS66VEo2s7xu",
      "authorship_tag": "ABX9TyMygDj5M2YccRiP8yuBdwly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyeon2776/98/blob/master/Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWXXRpVNCJo1"
      },
      "source": [
        "pip install dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xw8aHfBCf2X"
      },
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "def load_train(train_path, image_size, classes):\r\n",
        "    images = []\r\n",
        "    labels = []\r\n",
        "    ids = []\r\n",
        "    cls = []\r\n",
        "\r\n",
        "    print('Reading training images')\r\n",
        "    for fld in classes:\r\n",
        "        index = classes.index(fld)\r\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\r\n",
        "        path = os.path.join(train_path, fld, '*g')\r\n",
        "        files = glob.glob(path)\r\n",
        "        for fl in files:\r\n",
        "            image = cv2.imread(fl)\r\n",
        "            image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\r\n",
        "            images.append(image)\r\n",
        "            label = np.zeros(len(classes))\r\n",
        "            label[index] = 1.0\r\n",
        "            labels.append(label)\r\n",
        "            flbase = os.path.basename(fl)\r\n",
        "            ids.append(flbase)\r\n",
        "            cls.append(fld)\r\n",
        "    images = np.array(images)\r\n",
        "    labels = np.array(labels)\r\n",
        "    ids = np.array(ids)\r\n",
        "    cls = np.array(cls)\r\n",
        "  \r\n",
        "\r\n",
        "    return images, labels, ids, cls\r\n",
        "\r\n",
        "\r\n",
        "def load_test(test_path, image_size):\r\n",
        "    path = os.path.join(test_path, '*g')\r\n",
        "    files = sorted(glob.glob(path))\r\n",
        "\r\n",
        "    X_test = []\r\n",
        "    X_test_id = []\r\n",
        "    print(\"Reading test images\")\r\n",
        "    for fl in files:\r\n",
        "        \r\n",
        "        img = cv2.imread(fl)\r\n",
        "        img = cv2.resize(img, (image_size, image_size), cv2.INTER_LINEAR)\r\n",
        "        X_test.append(img)\r\n",
        "        X_test_id.append(flbase)\r\n",
        "        flbase = os.path.basename(fl)\r\n",
        "\r\n",
        "    X_test = np.array(X_test, dtype=np.uint8)\r\n",
        "    X_test = X_test.astype('float32')\r\n",
        "    X_test = X_test / 255\r\n",
        "    X_test_id = np.array(X_test_id)\r\n",
        "\r\n",
        "    return X_test, X_test_id\r\n",
        "\r\n",
        "\r\n",
        "class DataSet(object):\r\n",
        "\r\n",
        "    def __init__(self, images, labels, ids, cls):\r\n",
        "        \"\"\"Construct a DataSet. one_hot arg is used only if fake_data is true.\"\"\"\r\n",
        "\r\n",
        "        self._num_examples = images.shape[0]\r\n",
        "\r\n",
        "        # Convert shape from [num examples, rows, columns, depth]\r\n",
        "        # to [num examples, rows*columns] (assuming depth == 1)\r\n",
        "        # Convert from [0, 255] -> [0.0, 1.0].\r\n",
        "\r\n",
        "        images = images.astype(np.float32)\r\n",
        "        images = np.multiply(images, 1.0 / 255.0)\r\n",
        "\r\n",
        "        self._images = images\r\n",
        "        self._labels = labels\r\n",
        "        self._ids = ids\r\n",
        "        self._cls = cls\r\n",
        "        self._epochs_completed = 0\r\n",
        "        self._index_in_epoch = 0\r\n",
        "\r\n",
        "    @property\r\n",
        "    def images(self):\r\n",
        "        return self._images\r\n",
        "\r\n",
        "    @property\r\n",
        "    def labels(self):\r\n",
        "        return self._labels\r\n",
        "\r\n",
        "    @property\r\n",
        "    def ids(self):\r\n",
        "        return self._ids\r\n",
        "\r\n",
        "    @property\r\n",
        "    def cls(self):\r\n",
        "        return self._cls\r\n",
        "\r\n",
        "    @property\r\n",
        "    def num_examples(self):\r\n",
        "        return self._num_examples\r\n",
        "\r\n",
        "    @property\r\n",
        "    def epochs_completed(self):\r\n",
        "        return self._epochs_completed\r\n",
        "\r\n",
        "    def next_batch(self, batch_size):\r\n",
        "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\r\n",
        "        start = self._index_in_epoch\r\n",
        "        self._index_in_epoch += batch_size\r\n",
        "\r\n",
        "        if self._index_in_epoch > self._num_examples:\r\n",
        "            # Finished epoch\r\n",
        "            self._epochs_completed += 1\r\n",
        "\r\n",
        "            # # Shuffle the data (maybe)\r\n",
        "            # perm = np.arange(self._num_examples)\r\n",
        "            # np.random.shuffle(perm)\r\n",
        "            # self._images = self._images[perm]\r\n",
        "            # self._labels = self._labels[perm]\r\n",
        "            # Start next epoch\r\n",
        "\r\n",
        "            start = 0\r\n",
        "            self._index_in_epoch = batch_size\r\n",
        "            assert batch_size <= self._num_examples\r\n",
        "        end = self._index_in_epoch\r\n",
        "\r\n",
        "        return self._images[start:end], self._labels[start:end], self._ids[start:end], self._cls[start:end]\r\n",
        "\r\n",
        "\r\n",
        "def read_train_sets(train_path, image_size, classes, validation_size=0):\r\n",
        "    class DataSets(object):\r\n",
        "        pass\r\n",
        "\r\n",
        "    data_sets = DataSets()\r\n",
        "\r\n",
        "    images, labels, ids, cls = load_train(train_path, image_size, classes)\r\n",
        "    images, labels, ids, cls = shuffle(images, labels, ids, cls)  # shuffle the data\r\n",
        "\r\n",
        "    if isinstance(validation_size, float):\r\n",
        "        validation_size = int(validation_size * images.shape[0])\r\n",
        "\r\n",
        "    train_images = images\r\n",
        "    train_labels = labels\r\n",
        "    train_ids = ids\r\n",
        "    train_cls = cls\r\n",
        "\r\n",
        "    data_sets.train = DataSet(train_images, train_labels, train_ids, train_cls)\r\n",
        "\r\n",
        "    return data_sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XivseF1ECgL4"
      },
      "source": [
        "def load_validation(validation_path, image_size, classes):\r\n",
        "    images2 = []\r\n",
        "    labels2 = []\r\n",
        "    ids2 = []\r\n",
        "    cls2 = []\r\n",
        "\r\n",
        "    print('Reading validation images')\r\n",
        "    for fld in classes:\r\n",
        "        index = classes.index(fld)\r\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\r\n",
        "        path = os.path.join(validation_path, fld, '*g')\r\n",
        "        files = glob.glob(path)\r\n",
        "        for fl in files:\r\n",
        "            image = cv2.imread(fl)\r\n",
        "            image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\r\n",
        "            images2.append(image)\r\n",
        "            label = np.zeros(len(classes))\r\n",
        "            label[index] = 1.0\r\n",
        "            labels2.append(label)\r\n",
        "            flbase = os.path.basename(fl)\r\n",
        "            ids2.append(flbase)\r\n",
        "            cls2.append(fld)\r\n",
        "    images2 = np.array(images2)\r\n",
        "    labels2 = np.array(labels2)\r\n",
        "    ids2 = np.array(ids2)\r\n",
        "    cls2 = np.array(cls2)\r\n",
        "  \r\n",
        "\r\n",
        "    return images2, labels2, ids2, cls2\r\n",
        "\r\n",
        "\r\n",
        "def read_validation_sets(validation_path, image_size, classes, validation_size=0):\r\n",
        "    class DataSets(object):\r\n",
        "        pass\r\n",
        "\r\n",
        "    data_sets = DataSets()\r\n",
        "\r\n",
        "    images2, labels2, ids2, cls2 = load_validation(validation_path, image_size, classes)\r\n",
        "    images2, labels2, ids2, cls2 = shuffle(images2, labels2, ids2, cls2)  # shuffle the data\r\n",
        "\r\n",
        "    if isinstance(validation_size, float):\r\n",
        "        validation_size = int(validation_size * images2.shape[0])\r\n",
        "\r\n",
        "    validation_images = images2\r\n",
        "    validation_labels = labels2\r\n",
        "    validation_ids = ids2\r\n",
        "    validation_cls = cls2\r\n",
        "\r\n",
        "    data_sets.valid = DataSet(validation_images, validation_labels, validation_ids, validation_cls)\r\n",
        "\r\n",
        "    return data_sets\r\n",
        "\r\n",
        "def load_test(test_path, image_size, classes):\r\n",
        "    images3 = []\r\n",
        "    labels3 = []\r\n",
        "    ids3 = []\r\n",
        "    cls3 = []\r\n",
        "\r\n",
        "    print('Reading validation images')\r\n",
        "    for fld in classes:\r\n",
        "        index = classes.index(fld)\r\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\r\n",
        "        path = os.path.join(test_path, fld, '*g')\r\n",
        "        files = glob.glob(path)\r\n",
        "        for fl in files:\r\n",
        "            image = cv2.imread(fl)\r\n",
        "            image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\r\n",
        "            images3.append(image)\r\n",
        "            label = np.zeros(len(classes))\r\n",
        "            label[index] = 1.0\r\n",
        "            labels3.append(label)\r\n",
        "            flbase = os.path.basename(fl)\r\n",
        "            ids3.append(flbase)\r\n",
        "            cls3.append(fld)\r\n",
        "    images3 = np.array(images3)\r\n",
        "    labels3 = np.array(labels3)\r\n",
        "    ids3 = np.array(ids3)\r\n",
        "    cls3 = np.array(cls3)\r\n",
        "  \r\n",
        "\r\n",
        "    return images3, labels3, ids3, cls3\r\n",
        "\r\n",
        "\r\n",
        "def read_test_sets(test_path, image_size, classes, validation_size=0):\r\n",
        "    class DataSets(object):\r\n",
        "        pass\r\n",
        "\r\n",
        "    data_sets = DataSets()\r\n",
        "\r\n",
        "    images3, labels3, ids3, cls3 = load_test(test_path, image_size, classes)\r\n",
        "    images3, labels3, ids3, cls3 = shuffle(images3, labels3, ids3, cls3)  # shuffle the data\r\n",
        "\r\n",
        "    if isinstance(validation_size, float):\r\n",
        "        validation_size = int(validation_size * images3.shape[0])\r\n",
        "\r\n",
        "    test_images = images3\r\n",
        "    test_labels = labels3\r\n",
        "    test_ids = ids3\r\n",
        "    test_cls = cls3\r\n",
        "\r\n",
        "    data_sets.test = DataSet(test_images, test_labels, test_ids, test_cls)\r\n",
        "\r\n",
        "    return data_sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmNQajYZCgYu"
      },
      "source": [
        "import time\r\n",
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "import cv2\r\n",
        "import dataset\r\n",
        "import os\r\n",
        "import keras\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from datetime import timedelta\r\n",
        "import seaborn as sn\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "tf.disable_v2_behavior()\r\n",
        "\r\n",
        "filter_size0 = 3\r\n",
        "num_filters0 = 3\r\n",
        "\r\n",
        "filter_size1 = 3\r\n",
        "num_filters1 = 64\r\n",
        "\r\n",
        "filter_size2 = 3\r\n",
        "num_filters2 = 128\r\n",
        "\r\n",
        "filter_size3 = 3\r\n",
        "num_filters3 = 256\r\n",
        "\r\n",
        "filter_size4 = 3\r\n",
        "num_filters4 = 512\r\n",
        "\r\n",
        "filter_size5 = 3\r\n",
        "num_filters5 = 512\r\n",
        "\r\n",
        "filter_size6 = 7\r\n",
        "num_filters6 = 512\r\n",
        "\r\n",
        "filter_size7 = 3\r\n",
        "num_filters7 = 1024\r\n",
        "\r\n",
        "filter_size8 = 3\r\n",
        "num_filters8 = 2048\r\n",
        "\r\n",
        "# Fully-connected layer.\r\n",
        "fc_size = 4096\r\n",
        "fc_size2 = 1000    \r\n",
        "fc_size3 = 64       \r\n",
        "\r\n",
        "# Number of color channels for the images: 1 channel for gray-scale.\r\n",
        "num_channels = 3\r\n",
        "\r\n",
        "# image dimensions (only squares for now)\r\n",
        "img_size = 150\r\n",
        "\r\n",
        "# Size of image when flattened to a single dimension\r\n",
        "img_size_flat = img_size * img_size * num_channels\r\n",
        "\r\n",
        "# Tuple with height and width of images used to reshape arrays.\r\n",
        "img_shape = (img_size, img_size)\r\n",
        "\r\n",
        "# class info\r\n",
        "classes = ['1.Cancer', '2.Precancer', '3.Inflammatory', '4.Normal']\r\n",
        "num_classes = len(classes)\r\n",
        "\r\n",
        "# batch size\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# validation split\r\n",
        "validation_size = 0\r\n",
        "\r\n",
        "# how long to wait after validation loss stops improving before terminating training\r\n",
        "early_stopping = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuAtVHUdCgk2"
      },
      "source": [
        "train_path = '/content/drive/Shareddrives/CTRC-OralDetect-Project/NewDataGenerate-HM/train'\r\n",
        "validation_path = '/content/drive/Shareddrives/CTRC-OralDetect-Project/NewDataGenerate-HM/validation'\r\n",
        "test_path = '/content/drive/Shareddrives/CTRC-OralDetect-Project/NewDataGenerate-HM/test'\r\n",
        "checkpoint_dir = '/content/drive/MyDrive/model2/'\r\n",
        "\r\n",
        "data = read_train_sets(train_path, img_size, classes, validation_size=validation_size)\r\n",
        "data2 = read_validation_sets(validation_path, img_size, classes, validation_size=validation_size)\r\n",
        "data3 = read_test_sets(test_path, img_size, classes, validation_size=validation_size)\r\n",
        "\r\n",
        "\r\n",
        "print(\"Size of:\")\r\n",
        "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\r\n",
        "print(\"- Validation-set:\\t{}\".format(len(data2.valid.labels)))\r\n",
        "print(\"- Test-set:\\t\\t{}\".format(len(data3.test.labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iViccDxeCqRb"
      },
      "source": [
        "def new_weights(shape):\r\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\r\n",
        "\r\n",
        "def new_biases(length):\r\n",
        "    return tf.Variable(tf.constant(0.05, shape=[length]))\r\n",
        "\r\n",
        "def new_conv_layer_2048(input,              \r\n",
        "                   num_input_channels, \r\n",
        "                   filter_size,       \r\n",
        "                   num_filters,       \r\n",
        "                   use_pooling=False): \r\n",
        "\r\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\r\n",
        "\r\n",
        "    weights = new_weights(shape=shape)\r\n",
        "\r\n",
        "    biases = new_biases(length=num_filters)\r\n",
        "\r\n",
        "    layer = tf.nn.conv2d(input=input,\r\n",
        "                         filter=weights,\r\n",
        "                         strides=[1, 1, 1, 1],\r\n",
        "                         padding='SAME')\r\n",
        "\r\n",
        "    layer += biases\r\n",
        "\r\n",
        "    if use_pooling:\r\n",
        "\r\n",
        "        layer = tf.nn.max_pool(value=layer,\r\n",
        "                               ksize=[1, 2, 2, 1],\r\n",
        "                               strides=[1, 2, 2, 1],\r\n",
        "                               padding='SAME')\r\n",
        "\r\n",
        "    layer = tf.nn.relu(layer)\r\n",
        "\r\n",
        "    return layer, weights\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def new_conv_layer_2048_2(input,              \r\n",
        "                   num_input_channels, \r\n",
        "                   filter_size,       \r\n",
        "                   num_filters,       \r\n",
        "                   use_pooling=True): \r\n",
        "\r\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\r\n",
        "\r\n",
        "    weights = new_weights(shape=shape)\r\n",
        "\r\n",
        "    biases = new_biases(length=num_filters)\r\n",
        "\r\n",
        "    layer = tf.nn.conv2d(input=input,\r\n",
        "                         filter=weights,\r\n",
        "                         strides=[1, 1, 1, 1],\r\n",
        "                         padding='SAME')\r\n",
        "\r\n",
        "    layer += biases\r\n",
        "\r\n",
        "    if use_pooling:\r\n",
        "\r\n",
        "        layer = tf.nn.max_pool(value=layer,\r\n",
        "                               ksize=[1, 2, 2, 1],\r\n",
        "                               strides=[1, 2, 2, 1],\r\n",
        "                               padding='SAME')\r\n",
        "\r\n",
        "    layer = tf.nn.relu(layer)\r\n",
        "\r\n",
        "    return layer, weights\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def new_conv_layer_1024(input,              \r\n",
        "                   num_input_channels, \r\n",
        "                   filter_size,       \r\n",
        "                   num_filters,       \r\n",
        "                   use_pooling=False): \r\n",
        "\r\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\r\n",
        "\r\n",
        "    weights = new_weights(shape=shape)\r\n",
        "\r\n",
        "    biases = new_biases(length=num_filters)\r\n",
        "\r\n",
        "    layer = tf.nn.conv2d(input=input,\r\n",
        "                         filter=weights,\r\n",
        "                         strides=[1, 1, 1, 1],\r\n",
        "                         padding='SAME')\r\n",
        "\r\n",
        "    layer += biases  \r\n",
        "\r\n",
        "    if use_pooling:\r\n",
        "\r\n",
        "        layer = tf.nn.max_pool(value=layer,\r\n",
        "                               ksize=[1, 2, 2, 1],\r\n",
        "                               strides=[1, 2, 2, 1],\r\n",
        "                               padding='SAME')\r\n",
        "\r\n",
        "    layer = tf.nn.relu(layer)\r\n",
        "\r\n",
        "    return layer, weights\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def new_conv_layer_512(input,              \r\n",
        "                   num_input_channels, \r\n",
        "                   filter_size,       \r\n",
        "                   num_filters,       \r\n",
        "                   use_pooling=False): \r\n",
        "\r\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\r\n",
        "\r\n",
        "    weights = new_weights(shape=shape)\r\n",
        "\r\n",
        "    biases = new_biases(length=num_filters)\r\n",
        "\r\n",
        "    layer = tf.nn.conv2d(input=input,\r\n",
        "                         filter=weights,\r\n",
        "                         strides=[1, 1, 1, 1],\r\n",
        "                         padding='SAME')\r\n",
        "\r\n",
        "    layer += biases  \r\n",
        "\r\n",
        "    if use_pooling:\r\n",
        "\r\n",
        "        layer = tf.nn.max_pool(value=layer,\r\n",
        "                               ksize=[1, 2, 2, 1],\r\n",
        "                               strides=[1, 2, 2, 1],\r\n",
        "                               padding='SAME')\r\n",
        "\r\n",
        "    layer = tf.nn.relu(layer)\r\n",
        "\r\n",
        "    return layer, weights\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def new_conv_layer_256(input,              \r\n",
        "                   num_input_channels, \r\n",
        "                   filter_size,       \r\n",
        "                   num_filters,       \r\n",
        "                   use_pooling=False): \r\n",
        "\r\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\r\n",
        "\r\n",
        "    weights = new_weights(shape=shape)\r\n",
        "\r\n",
        "    biases = new_biases(length=num_filters)\r\n",
        "\r\n",
        "    layer = tf.nn.conv2d(input=input,\r\n",
        "                         filter=weights,\r\n",
        "                         strides=[1, 1, 1, 1],\r\n",
        "                         padding='SAME')\r\n",
        "\r\n",
        "    layer += biases  \r\n",
        "\r\n",
        "    if use_pooling:\r\n",
        "\r\n",
        "        layer = tf.nn.max_pool(value=layer,\r\n",
        "                               ksize=[1, 2, 2, 1],\r\n",
        "                               strides=[1, 2, 2, 1],\r\n",
        "                               padding='SAME')\r\n",
        "\r\n",
        "    layer = tf.nn.relu(layer)\r\n",
        "\r\n",
        "    return layer, weights\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def new_conv_layer_128(input,              \r\n",
        "                   num_input_channels, \r\n",
        "                   filter_size,       \r\n",
        "                   num_filters,       \r\n",
        "                   use_pooling=False): \r\n",
        "\r\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\r\n",
        "\r\n",
        "    weights = new_weights(shape=shape)\r\n",
        "\r\n",
        "    biases = new_biases(length=num_filters)\r\n",
        "\r\n",
        "    layer = tf.nn.conv2d(input=input,\r\n",
        "                         filter=weights,\r\n",
        "                         strides=[1, 1, 1, 1],\r\n",
        "                         padding='SAME')\r\n",
        "\r\n",
        "    layer += biases  \r\n",
        "\r\n",
        "    if use_pooling:\r\n",
        "\r\n",
        "        layer = tf.nn.max_pool(value=layer,\r\n",
        "                               ksize=[1, 2, 2, 1],\r\n",
        "                               strides=[1, 2, 2, 1],\r\n",
        "                               padding='SAME')\r\n",
        "\r\n",
        "    layer = tf.nn.relu(layer)\r\n",
        "\r\n",
        "    return layer, weights\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def flatten_layer(layer):\r\n",
        "\r\n",
        "    layer_shape = layer.get_shape()\r\n",
        "\r\n",
        "    num_features = layer_shape[1:8].num_elements()\r\n",
        "    \r\n",
        "    layer_flat = tf.reshape(layer, [-1, num_features])\r\n",
        "\r\n",
        "    return layer_flat, num_features\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def new_fc_layer(input,        \r\n",
        "                 num_inputs,    \r\n",
        "                 num_outputs,  \r\n",
        "                 use_relu=True):\r\n",
        "\r\n",
        "    weights = new_weights(shape=[num_inputs, num_outputs])\r\n",
        "    biases = new_biases(length=num_outputs)\r\n",
        "\r\n",
        "    layer = tf.matmul(input, weights) + biases\r\n",
        "    \r\n",
        "    if use_relu:\r\n",
        "        layer = tf.nn.relu(layer)\r\n",
        "                \r\n",
        "    return layer\r\n",
        "\r\n",
        "\r\n",
        "def new_fc_layer2(input,        \r\n",
        "                 num_inputs,    \r\n",
        "                 num_outputs,  \r\n",
        "                 use_relu=True):\r\n",
        "\r\n",
        "    weights = new_weights(shape=[num_inputs, num_outputs])\r\n",
        "    biases = new_biases(length=num_outputs)\r\n",
        "\r\n",
        "    layer = tf.matmul(input, weights) + biases\r\n",
        "    \r\n",
        "    if use_relu:\r\n",
        "        layer = tf.nn.relu(layer)\r\n",
        "        layer = tf.nn.dropout(layer, keep_prob=keep_prob)\r\n",
        "                \r\n",
        "    return layer\r\n",
        "\r\n",
        "x = tf.compat.v1.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\r\n",
        "\r\n",
        "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\r\n",
        "\r\n",
        "\r\n",
        "y_true = tf.compat.v1.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\r\n",
        "\r\n",
        "y_true_cls = tf.argmax(y_true, axis=1)\r\n",
        "\r\n",
        "keep_prob = tf.compat.v1.placeholder(tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWH5q5H6Cqaj"
      },
      "source": [
        "layer_conv1, weights_conv1 = \\\r\n",
        "    new_conv_layer_2048(input=x_image,\r\n",
        "                   num_input_channels=num_channels,\r\n",
        "                   filter_size=filter_size8,\r\n",
        "                   num_filters=num_filters8,\r\n",
        "                   use_pooling=False)\r\n",
        "     \r\n",
        "layer_conv2, weights_conv2 = \\\r\n",
        "    new_conv_layer_2048_2(input=layer_conv1,\r\n",
        "                   num_input_channels=num_filters8,\r\n",
        "                   filter_size=filter_size8,\r\n",
        "                   num_filters=num_filters8,\r\n",
        "                   use_pooling=True)\r\n",
        "    \r\n",
        "layer_conv3, weights_conv3 = \\\r\n",
        "    new_conv_layer_1024(input=layer_conv2,\r\n",
        "                   num_input_channels=num_filters8,\r\n",
        "                   filter_size=filter_size7,\r\n",
        "                   num_filters=num_filters7,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv4, weights_conv4 = \\\r\n",
        "    new_conv_layer_1024(input=layer_conv3,\r\n",
        "                   num_input_channels=num_filters7,\r\n",
        "                   filter_size=filter_size7,\r\n",
        "                   num_filters=num_filters7,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv5, weights_conv5 = \\\r\n",
        "    new_conv_layer_1024(input=layer_conv4,\r\n",
        "                   num_input_channels=num_filters7,\r\n",
        "                   filter_size=filter_size7,\r\n",
        "                   num_filters=num_filters7,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv6, weights_conv6 = \\\r\n",
        "    new_conv_layer_1024(input=layer_conv5,\r\n",
        "                   num_input_channels=num_filters7,\r\n",
        "                   filter_size=filter_size7,\r\n",
        "                   num_filters=num_filters7,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv7, weights_conv7 = \\\r\n",
        "    new_conv_layer_1024(input=layer_conv6,\r\n",
        "                   num_input_channels=num_filters7,\r\n",
        "                   filter_size=filter_size7,\r\n",
        "                   num_filters=num_filters7,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv8, weights_conv8 = \\\r\n",
        "    new_conv_layer_1024(input=layer_conv7,\r\n",
        "                   num_input_channels=num_filters7,\r\n",
        "                   filter_size=filter_size7,\r\n",
        "                   num_filters=num_filters7,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv9, weights_conv9 = \\\r\n",
        "    new_conv_layer_1024(input=layer_conv8,\r\n",
        "                   num_input_channels=num_filters7,\r\n",
        "                   filter_size=filter_size7,\r\n",
        "                   num_filters=num_filters7,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv10, weights_conv10 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv9,\r\n",
        "                   num_input_channels=num_filters7,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv11, weights_conv11 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv10,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv12, weights_conv12 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv11,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv13, weights_conv13 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv12,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv14, weights_conv14 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv13,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv15, weights_conv15 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv14,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv16, weights_conv16 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv15,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv17, weights_conv17 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv16,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv18, weights_conv18 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv17,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv19, weights_conv19 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv18,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv20, weights_conv20 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv19,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv21, weights_conv21 = \\\r\n",
        "    new_conv_layer_512(input=layer_conv20,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size5,\r\n",
        "                   num_filters=num_filters5,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv22, weights_conv22 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv21,\r\n",
        "                   num_input_channels=num_filters5,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv23, weights_conv23 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv22,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv24, weights_conv24 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv23,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv25, weights_conv25 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv24,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv26, weights_conv26 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv25,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv27, weights_conv27 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv26,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv28, weights_conv28 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv27,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv29, weights_conv29 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv28,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv30, weights_conv30 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv29,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv31, weights_conv31 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv30,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv32, weights_conv32 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv31,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv33, weights_conv33 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv32,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv34, weights_conv34 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv33,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv35, weights_conv35 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv34,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv36, weights_conv36 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv35,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv37, weights_conv37 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv36,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv38, weights_conv38 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv37,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv39, weights_conv39 = \\\r\n",
        "    new_conv_layer_256(input=layer_conv38,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size3,\r\n",
        "                   num_filters=num_filters3,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv40, weights_conv40 = \\\r\n",
        "    new_conv_layer_128(input=layer_conv39,\r\n",
        "                   num_input_channels=num_filters3,\r\n",
        "                   filter_size=filter_size2,\r\n",
        "                   num_filters=num_filters2,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv41, weights_conv41 = \\\r\n",
        "    new_conv_layer_128(input=layer_conv40,\r\n",
        "                   num_input_channels=num_filters2,\r\n",
        "                   filter_size=filter_size2,\r\n",
        "                   num_filters=num_filters2,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv42, weights_conv42 = \\\r\n",
        "    new_conv_layer_128(input=layer_conv41,\r\n",
        "                   num_input_channels=num_filters2,\r\n",
        "                   filter_size=filter_size2,\r\n",
        "                   num_filters=num_filters2,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv43, weights_conv43 = \\\r\n",
        "    new_conv_layer_128(input=layer_conv42,\r\n",
        "                   num_input_channels=num_filters2,\r\n",
        "                   filter_size=filter_size2,\r\n",
        "                   num_filters=num_filters2,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv44, weights_conv44 = \\\r\n",
        "    new_conv_layer_128(input=layer_conv43,\r\n",
        "                   num_input_channels=num_filters2,\r\n",
        "                   filter_size=filter_size2,\r\n",
        "                   num_filters=num_filters2,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv45, weights_conv45 = \\\r\n",
        "    new_conv_layer_128(input=layer_conv44,\r\n",
        "                   num_input_channels=num_filters2,\r\n",
        "                   filter_size=filter_size2,\r\n",
        "                   num_filters=num_filters2,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_conv46, weights_conv46 = \\\r\n",
        "    new_conv_layer_128(input=layer_conv45,\r\n",
        "                   num_input_channels=num_filters2,\r\n",
        "                   filter_size=filter_size2,\r\n",
        "                   num_filters=num_filters2,\r\n",
        "                   use_pooling=False)\r\n",
        "    \r\n",
        "layer_flat, num_features = flatten_layer(layer_conv46)\r\n",
        "\r\n",
        "layer_fc1 = new_fc_layer(input=layer_flat,\r\n",
        "                         num_inputs=num_features,\r\n",
        "                         num_outputs=64,\r\n",
        "                         use_relu=True)\r\n",
        "\r\n",
        "layer_fc2 = new_fc_layer(input=layer_fc1,\r\n",
        "                         num_inputs=64,\r\n",
        "                         num_outputs=num_channels,\r\n",
        "                         use_relu=True)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "y_pred = tf.nn.softmax(layer_fc2)\r\n",
        "\r\n",
        "y_pred_cls = tf.argmax(y_pred, axis=1)\r\n",
        "\r\n",
        "loss_func = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc2, labels=y_true)\r\n",
        "\r\n",
        "cost = tf.reduce_mean(loss_func)\r\n",
        "\r\n",
        "regularizer = tf.nn.l2_loss(weights_conv46)\r\n",
        "beta = 0.001\r\n",
        "cost = tf.reduce_mean(cost + beta*regularizer)\r\n",
        "\r\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\r\n",
        "\r\n",
        "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\r\n",
        "\r\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP-JZY8mCqhE"
      },
      "source": [
        "session = tf.Session()\r\n",
        "\r\n",
        "session.run(tf.global_variables_initializer())\r\n",
        "\r\n",
        "train_batch_size = batch_size\r\n",
        "\r\n",
        "def print_progress(epoch, feed_dict_train, val_loss):\r\n",
        "    acc = session.run(accuracy, feed_dict=feed_dict_train)\r\n",
        "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%},  Validation Loss: {2:.3f}\"\r\n",
        "    print(msg.format(epoch + 1, acc, val_loss))\r\n",
        "\r\n",
        "    if epoch <= 48:\r\n",
        "      print(print_validation_accuracy(show_confusion_matrix=True))\r\n",
        "      print(print_test_accuracy(show_confusion_matrix=True))\r\n",
        "    else:\r\n",
        "      print(print_validation_accuracy(show_confusion_matrix=True))\r\n",
        "      print(print_test_accuracy(show_confusion_matrix=True))\r\n",
        "\r\n",
        "\r\n",
        "total_iterations = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w687ZUugCqnX"
      },
      "source": [
        "def optimize(num_iterations):\r\n",
        "    global total_iterations\r\n",
        "    \r\n",
        "\r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    best_val_loss = float(\"inf\")\r\n",
        "    patience = 0\r\n",
        "\r\n",
        "    for i in range(total_iterations,\r\n",
        "                   total_iterations + num_iterations):\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\r\n",
        "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data2.valid.next_batch(train_batch_size)\r\n",
        "        x_test_batch, y_test_batch, _, test_cls_batch = data3.test.next_batch(train_batch_size)\r\n",
        "\r\n",
        "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\r\n",
        "        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\r\n",
        "        x_test_batch = x_test_batch.reshape(train_batch_size, img_size_flat)\r\n",
        "\r\n",
        "        feed_dict_train = {x: x_batch,\r\n",
        "                           y_true: y_true_batch}\r\n",
        "        \r\n",
        "        feed_dict_validate = {x: x_valid_batch,\r\n",
        "                              y_true: y_valid_batch}\r\n",
        "\r\n",
        "        feed_dict_test = {x: x_test_batch,\r\n",
        "                          y_true: y_test_batch}\r\n",
        "\r\n",
        "        session.run(optimizer, feed_dict=feed_dict_train)\r\n",
        "        \r\n",
        "        if i % int(data.train.num_examples/batch_size) == 0: \r\n",
        "            val_loss = session.run(cost, feed_dict=feed_dict_validate)\r\n",
        "            epoch = int(i / int(data.train.num_examples/batch_size))\r\n",
        "\r\n",
        "            print_progress(epoch, feed_dict_train, val_loss)\r\n",
        "            \r\n",
        "            if early_stopping:    \r\n",
        "                if val_loss < best_val_loss:\r\n",
        "                    best_val_loss = val_loss\r\n",
        "                    patience = 0\r\n",
        "                else:\r\n",
        "                    patience += 1\r\n",
        "\r\n",
        "                if patience == early_stopping:\r\n",
        "                    break\r\n",
        "\r\n",
        "\r\n",
        "    total_iterations += num_iterations\r\n",
        "\r\n",
        "    end_time = time.time()\r\n",
        "\r\n",
        "    time_dif = end_time - start_time\r\n",
        "\r\n",
        "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYKuhWRDCq2F"
      },
      "source": [
        "def print_validation_accuracy(show_example_errors=False, show_confusion_matrix=False):\r\n",
        "\r\n",
        "    num_test = len(data2.valid.images)\r\n",
        "\r\n",
        "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\r\n",
        "\r\n",
        "    i = 0\r\n",
        "\r\n",
        "    while i < num_test:\r\n",
        " \r\n",
        "        j = min(i + batch_size, num_test)\r\n",
        "\r\n",
        "        images = data2.valid.images[i:j, :].reshape(-1, img_size_flat)\r\n",
        "        \r\n",
        "        labels = data2.valid.labels[i:j, :]\r\n",
        "\r\n",
        "        feed_dict = {x: images, y_true: labels}\r\n",
        "\r\n",
        "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\r\n",
        "\r\n",
        "        i = j\r\n",
        "\r\n",
        "\r\n",
        "    cls_true = np.array(data2.valid.cls)\r\n",
        "    cls_pred = np.array([classes[x] for x in cls_pred])\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "    correct = (cls_true == cls_pred)\r\n",
        "\r\n",
        "    correct_sum = correct.sum()\r\n",
        "\r\n",
        "    acc = float(correct_sum) / num_test\r\n",
        "\r\n",
        "    msg = \"Validation Accuracy: {0:.1%} ({1} / {2})\"\r\n",
        "    print(msg.format(acc, correct_sum, num_test))\r\n",
        "\r\n",
        "    if show_example_errors:\r\n",
        "        print(\"Example errors:\")\r\n",
        "        plot_example_errors(cls_pred=cls_pred, correct=correct)\r\n",
        "\r\n",
        "    if show_confusion_matrix:\r\n",
        "        print(\"Confusion Matrix:\")\r\n",
        "        plot_confusion_matrix(cls_pred=cls_pred)\r\n",
        "\r\n",
        "def print_test_accuracy(show_example_errors=False, show_confusion_matrix=False):\r\n",
        "\r\n",
        "    num_test2 = len(data3.test.images)\r\n",
        "\r\n",
        "    cls_pred2 = np.zeros(shape=num_test2, dtype=np.int)\r\n",
        "\r\n",
        "    i = 0\r\n",
        "\r\n",
        "    while i < num_test2:\r\n",
        " \r\n",
        "        j = min(i + batch_size, num_test2)\r\n",
        "\r\n",
        "        images = data3.test.images[i:j, :].reshape(-1, img_size_flat)\r\n",
        "        \r\n",
        "        labels = data3.test.labels[i:j, :]\r\n",
        "\r\n",
        "        feed_dict = {x: images, y_true: labels}\r\n",
        "\r\n",
        "        cls_pred2[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\r\n",
        "\r\n",
        "        i = j\r\n",
        "\r\n",
        "\r\n",
        "    cls_true2 = np.array(data3.test.cls)\r\n",
        "    cls_pred2 = np.array([classes[x] for x in cls_pred2])\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "    correct = (cls_true2 == cls_pred2)\r\n",
        "\r\n",
        "    correct_sum = correct.sum()\r\n",
        "\r\n",
        "    acc = float(correct_sum) / num_test2\r\n",
        "\r\n",
        "    msg = \"Test Accuracy: {0:.1%} ({1} / {2})\"\r\n",
        "    print(msg.format(acc, correct_sum, num_test2))\r\n",
        "\r\n",
        "    if show_example_errors:\r\n",
        "        print(\"Example errors:\")\r\n",
        "        plot_example_errors(cls_pred2=cls_pred2, correct=correct)\r\n",
        "\r\n",
        "    if show_confusion_matrix:\r\n",
        "        print(\"Confusion Matrix:\")\r\n",
        "        plot_confusion_matrix2(cls_pred2=cls_pred2)\r\n",
        "\r\n",
        "\r\n",
        "def plot_confusion_matrix(cls_pred):\r\n",
        "    \r\n",
        "    cls_true = data2.valid.cls\r\n",
        "\r\n",
        "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\r\n",
        "    \r\n",
        "    print(cm)\r\n",
        "    \r\n",
        "    plt.matshow(cm, cmap=plt.cm.Wistia_r)\r\n",
        "\r\n",
        "    plt.colorbar()\r\n",
        "    tick_marks = np.arange(num_classes)\r\n",
        "    plt.xticks(tick_marks, range(num_classes))\r\n",
        "    plt.yticks(tick_marks, range(num_classes))\r\n",
        "    plt.grid(False)\r\n",
        "\r\n",
        "    plt.xlabel('Predicted')\r\n",
        "    plt.ylabel('True')\r\n",
        "    for i in range(0,4):\r\n",
        "      for j in range(0,4):\r\n",
        "        plt.text(j,i,str(cm[i][j]))\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    print(\"1.Cancer의 정밀도 : \", (cm[0,0]/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]))*100, \"%\")\r\n",
        "    print(\"1.Cancer의 재현율 : \", (cm[0,0]/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]))*100, \"%\")\r\n",
        "    print(\"2.Precancer의 정밀도 : \", (cm[1,1]/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]))*100, \"%\")\r\n",
        "    print(\"2.Precancer의 재현율 : \", (cm[1,1]/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]))*100, \"%\")\r\n",
        "    print(\"3.Inflammatory의 정밀도 : \", (cm[2,2]/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]))*100, \"%\")\r\n",
        "    print(\"3.Inflammatory의 재현율 : \", (cm[2,2]/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]))*100, \"%\")\r\n",
        "    print(\"4.Normal의 정밀도 : \", (cm[3,3]/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]))*100, \"%\")\r\n",
        "    print(\"4.Normal의 재현율 : \", (cm[3,3]/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]))*100, \"%\")\r\n",
        "\r\n",
        "\r\n",
        "def plot_confusion_matrix2(cls_pred2):\r\n",
        "    \r\n",
        "    cls_true2 = data3.test.cls\r\n",
        "\r\n",
        "    cm = confusion_matrix(y_true=cls_true2, y_pred=cls_pred2)\r\n",
        "    \r\n",
        "    print(cm)\r\n",
        "    \r\n",
        "    plt.matshow(cm, cmap=plt.cm.Wistia_r)\r\n",
        "\r\n",
        "    plt.colorbar()\r\n",
        "    \r\n",
        "    plt.title('Test Confusion Matrix')\r\n",
        "    tick_marks = np.arange(num_classes)\r\n",
        "    plt.xticks(tick_marks, range(num_classes))\r\n",
        "    plt.yticks(tick_marks, range(num_classes))\r\n",
        "    plt.grid(False)\r\n",
        "\r\n",
        "    plt.xlabel('Predicted')\r\n",
        "    plt.ylabel('True')\r\n",
        "    for i in range(0,4):\r\n",
        "      for j in range(0,4):\r\n",
        "        plt.text(j,i,str(cm[i][j]))\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    print(\"1.Cancer의 정밀도 : \", (cm[0,0]/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]))*100, \"%\")\r\n",
        "    print(\"1.Cancer의 재현율 : \", (cm[0,0]/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]))*100, \"%\")\r\n",
        "    print(\"2.Precancer의 정밀도 : \", (cm[1,1]/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]))*100, \"%\")\r\n",
        "    print(\"2.Precancer의 재현율 : \", (cm[1,1]/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]))*100, \"%\")\r\n",
        "    print(\"3.Inflammatory의 정밀도 : \", (cm[2,2]/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]))*100, \"%\")\r\n",
        "    print(\"3.Inflammatory의 재현율 : \", (cm[2,2]/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]))*100, \"%\")\r\n",
        "    print(\"4.Normal의 정밀도 : \", (cm[3,3]/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]))*100, \"%\")\r\n",
        "    print(\"4.Normal의 재현율 : \", (cm[3,3]/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]))*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbk5pnyBCz-o"
      },
      "source": [
        "optimize(num_iterations=1000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SJan3ViC0K7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxQLu8H8C0as"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxMdJoTkC0ky"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}